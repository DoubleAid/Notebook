# 后端2

## 主要目标

1. 理解 Pose Graph 优化
2. 理解因子图优化
3. 理解增量式图优化的工作原理

上一讲介绍了以BA为主的图优化，BA能精确的优化每一个相机的位姿和特征点位置，但是在实际场景中，由于特征点过多，导致计算量会越来越大，本讲会介绍两种更大场景中使用的后端优化方法：位姿图和因子图

## 位姿图

在实际场景中，计算的大部分时间都花在了计算观测方程上，而一个观测点经过几次观测后，他的位置优化已经收敛了，这时候再对这个点进行优化，效益不大，可以将他们看作一个固定的特征点，制作为位姿估计的约束，不再进行位置优化

就是因为BA的计算规模实在是太大了，如果实现实时的SLAM算法，必须要考虑一些解决方式

+ 像滑动窗口一样，丢弃一些历史数据
+ 项 PoseGraph一样舍弃对路标点的优化，只保留位姿之间的边

### 位姿图优化

在位姿图中，每一个节点都是一个相机的位姿，用 $\zeta_i$ 来表示, 那么就存在一个位姿的变化量$\Delta \zeta$使得位姿有$\zeta_i$过渡到$\zeta_j$
$$
\zeta_j = \zeta_i \Delta \zeta_{ij}
$$

在实际情况下，位姿是符合高斯分布的，直接作差不方便求出，我们对两边取对数，得到
$$
ln(\zeta_j) = ln(\zeta_i \Delta \zeta_{ij}) \\
ln(\Delta \zeta_{ij}) = ln(\zeta_j) - ln(\zeta_i) = ln(\frac {\zeta_j}{\zeta_i})
$$

这是我们可以构建误差方程
$$
e_{ij}^, = ln(\frac {\zeta_j}{\zeta_i}) - ln(\Delta \zeta_{ij}) = ln(\frac {\zeta_j}{\zeta_i \Delta \zeta_{ij}})
$$
此时我们需要优化的变量为 $\zeta_{i}$ 和 $\zeta_{j}$, 我们需要对两个参数求导

整个目标函数就变成了
$$
\min_{\zeta} \frac 1 2 \sum_{i,j} e_{ij}^TT_{ij}^{-1} e_{ij}^,
$$

我们依然可以用高斯牛顿法和列文伯格-马夸尔特方法等求解此问题，除了用李代数表示优化位姿以外，别的都是相似的

## 因子图优化初步

### 贝叶斯网络

我们从另一个角度来看后端优化，所谓的因子图优化(Factor Graph)。

从贝叶斯网络的角度来看，SLAM可以自然的表达成一个动态贝叶斯网络（Dynamic Bayes Network, DBN）。和图优化类似，贝叶斯网络是一个概率图，由随机变量的节点和表达随机变量条件独立性的边组成，形成一个有向无环图，在SLAM中，由于我们有运动方程和观测方程，他们恰好表示了状态变量之间的条件概率
这样 P_{k} 表示为 k 点的后验概率，P_{k}^, 表示k点的先验概率

那么整个贝叶斯网络可以表示为
$$
P_{k} = P(z_k | x_k)P(x_k | x_{k-1}, u_k) = P_0 \prod P(z_k | x_k) \prod P(x_k | x_{k-1}, u_k)
$$

