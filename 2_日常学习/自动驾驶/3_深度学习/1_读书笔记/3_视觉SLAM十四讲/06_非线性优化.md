# 非线性优化

## 主要目标

1. 理解最小二乘法的含义和处理方式
2. 理解高斯牛顿法，列文伯格-马夸尔特方法等下降策略

由前面已经知道，方程中的位姿可以由变换矩阵来描述，然后用李代数进行优化，观测方程由相机成像模型给出，内参是固定的，外参是相机的位姿

由于噪声的存在，上面所说的等式必定不是准确的，而是近似等式，因此需要对误差进行最小化，从而得到最优的位姿估计

## 状态估计问题

首先回顾一下经典SLAM模型，由一个状态方程和一个运动方程构成

$$
x_k = f(x_{k-1}, u_k) + w_k
z_{k, j} = h(y_j, x_k) + v_{k, j}
$$

这里的$x_k$ 是相机的位姿，我们可以使用变换矩阵或者李代数来表示，至于观测方程，也就是针孔相机模型，这里将讨论其具体的参数化形式

假设在 $x_k$ 处对路标 $y_j$ 进行了一次观测，对应到图像上的像素位置 $Z_{k,j}$ 那么观测方程为

$$
sZ_{k,j} = Kx_ky_j = Kexp(\theta_{\times})y_j
$$

其中，K为相机内参，s为单位像素的长度

数据受噪音影响后，在运动和观测方程中，通常假设两个噪声项 $W_k, V_kj$ 分别服从高斯分布，即

$$
w_k ～ N(0, R_k), v_k ～ N(0, Q_{k, j})
$$

在这些噪音的影响下，我们希望通过带噪音的数据z和u推断位姿x和地图y （以及他们的概率分布）这构成了一个状态估计问题

由于这些数据是随着时间逐渐到来的，所以在历史上很长一段时间内，研究者都是用滤波器，尤其是扩展卡尔曼滤波器（EKF）求解，

卡尔曼滤波器关心当前时刻的状态估计 $x_k$, 而对之前的状态则不多考虑
近些年来普遍使用的非线性优化方法，使用所有时刻采集到的数据进行状态估计，被认为优于传统的滤波器，也是当前视觉SLAM的主流方法

首先，在非线性优化中，把所有带估计的变量放在一个 "状态变量" 中

$x = {x_1, ..., x_N, y_1, ..., y_M}$

对机器人状态的估计，就是已知输入数据u和观测数据z的条件下，求计算状态x的条件概率分布

$$
P(x | z, u)
$$

类似于x，这里的u和z也是对所有数据的统称

对于没有测量运动的传感器，且不考虑图像在时间上的联系，只看图片(只要图片足够多) 就可以重建三维空间，这个问题称为 Structure from Motion (SfM)

在这种情况下，SLAM可以看作图像具有时间先后顺序，需要实时求解一个 SfM问题

为了估计状态变量的分布条件，利用贝叶斯法则，有

$$
P(x \mid z) = \frac {P(z \mid x) P(x)} {P(z)} \propto P(z \mid x) P(x)
$$

贝叶斯法则的左边通常为后验概率，右侧的 P(z \mid x)称为似然，另一部份 P(x) 称为先验

直接球后验分布是困难的，但是求一个状态最有估计，是的该状态下后验概率最大化是可行的

$$
x^*_{MAP} = argmax_x P(x \mid z) = argmax_x P(z \mid x) P(x)
$$

因为我们求的是位姿的一个估计分布，所以和分母，也就是z出现的概率无关，而只和分子有关，也就是似然和先验的乘积

但我们并不知道位姿的具体位置，也就是说没有了先验，那么求解x的最大似然估计，也就是求解似然函数的最大值 （Maximize Likelihood Estimation, MLE）

$$
x^*_{MLE} = argmax_x P(z \mid x)
$$

由于我们只知道观测数据，所以最大似然估计可以理解称为，在什么样的状态下，最可能产生现在观测到的数据

### 最小二乘的引出

如何求解最大似然估计呢，现在我们知道 观测模型，假设噪音项符合高斯分布

$$
z_{k, j} = h(y_j, x_k) + v_{k, j} \\
P(z_{k, j} \mid x_k) = \frac{1}{\sqrt{2\pi Q_{k, j}}} exp \left( -\frac{1}{2} \frac{(z_{k, j} - h(y_j, x_k))^T Q_{k, j}^{-1} (z_{k, j} - h(y_j, x_k))}{2} \right)
$$

可以看出，只要最小化似然函数的指数部分，就可以得到最大似然估计

$$
x^* = argmax_x P(z \mid x) = argmin_x  (z_{k, j} - h(y_j, x_k))^T Q_{k, j}^{-1} (z_{k, j} - h(y_j, x_k))
$$

可以看出，这个式子就是最小化噪音项的平方，因此，对于所有的运动和任意的观测，我们定义数据和估计值之间的误差为

$$
e_{v, k} = x_k - f(x_{k-1}, v_{k}) \\
e_{y, j, k} = z_{k, j} - h(x_k, y_j) \\
\\
J(x) = \sum_{k=1}^{N} \sum_{j=1}^{M} e_{y, j, k}^T Q_{k, j}^{-1} e_{y, j, k} + \sum_{k=1}^{N} e_{v, k}^T R_k^{-1} e_{v, k}
$$

其中，$R_k$ 和 $Q_{k, j}$ 分别是运动噪声和观测噪声的协方差矩阵

这样就得到了一个整体意义下的最小二乘问题，即最小化误差的平方和，他的最优解等价于状态的最大似然估计

直观的讲，由于噪声的存在，我们把估计的轨迹和地图代入SLAM的运动，观测方程中是，并不是完美的成立，而是近似成立的，我们需要对状态的估计值进行微调，使得整个运动或观测的误差下降一些，这个下降是有限度的，一般会达到一个最小值，这就是典型的非线性优化过程

观察最后的整体误差公式，我们可以了解到SLAM中的最小二乘问题有几个特点

+ 整个问题的目标函数是由许多误差的平方和构成的，每个误差项代表一个观测方程的误差，每个观测方程的误差项都与观测噪声的协方差矩阵有关
+ 每个误差项都是简单的，比如运动误差只和 $x_k-1$, $x_k$ 有关，观测误差只和 $x_k$, $y_j$ 有关

## 非线性最小二乘

非线性最小二乘问题是指目标函数是关于未知参数的非线性函数，而最小二乘法是一种求解非线性最小二乘问题的常用方法。在SLAM中，非线性最小二乘问题通常用于优化相机位姿和地图点的估计。

最小二乘问题（Least Squares Problem）是一种数学优化问题，其目标是通过调整一组参数，使观测值与预测值之间的误差平方和最小化。它是非线性优化中的一个重要分支，广泛应用于数据拟合、状态估计、机器学习和计算机视觉等领域。

### **1. 最小二乘问题的定义**

假设我们有一组观测数据 $(x_i, y_i)$ ，其中 $x_i$ 是输入变量，$y_i$ 是观测值。我们希望通过一个模型 $f(x, \theta)$ 来预测 $y_i$，其中 $\theta$ 是模型的参数。观测值与预测值之间的误差为：
$$
e_i = y_i - f(x_i, \theta)
$$
最小二乘问题的目标是找到一组参数 \(\theta^*\)，使得误差平方和最小：
$$
\theta^* = \arg\min_{\theta} \sum_{i=1}^n e_i^2 = \arg\min_{\theta} \sum_{i=1}^n (y_i - f(x_i, \theta))^2
$$

### **2. 最小二乘问题的分类**
根据模型 $f(x, \theta)$ 的形式，最小二乘问题可以分为以下两类：

#### **线性最小二乘问题**
- 模型 $f(x, \theta)$ 是参数 $\theta$ 的线性函数，例如：
  $$
  f(x, \theta) = \theta_0 + \theta_1 x + \theta_2 x^2 + \cdots + \theta_k x^k
  $$
- 这种情况下，问题可以通过线性代数的方法（如正规方程或奇异值分解）直接求解。

#### **非线性最小二乘问题**
- 模型 $f(x, \theta)$ 是参数 $\theta$ 的非线性函数，例如：
  $$
  f(x, \theta) = \theta_0 e^{\theta_1 x} + \theta_2 \sin(\theta_3 x)
  $$
- 这种情况下，通常需要使用迭代优化方法（如高斯牛顿法、列文伯格-马夸尔特法）来求解。

### **3. 非线性最小二乘问题的求解方法**

#### 牛顿法

求解这类问题最简单的方式就是通过泰勒公式展开
$$
f(x + \Delta x) \approx f(x) + f'(x) \Delta x + \frac{1}{2} \Delta x^Tf''(x) \Delta x
$$

将 $f^,(x)$ 就是x的导数 (雅可比矩阵)，$f^,(x)$ 就是x的二阶导数 (海森矩阵)

代入上面的公式，得到
$$
保留一阶梯度 \\
\Delta \theta = - J^T(x) \\
保留二阶梯度 \\
\Delta \theta = - \frac {J^T(x)} {H(x)}
$$

我们以一个实例举例，有一群观测点和观测的结果分别为 x = {1, 2, 3, 4, 5} 和 y = {2, 4, 9, 17, 20}，我们希望找到一个线性模型 $y = a_1x + a_2x^2 + a_3x^3$ 来拟合这些数据

首先，我们定义误差函数
$$
e_i = y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)
$$
然后，我们定义目标函数
$$
J(a_1, a_2, a_3) = \sum_{i=1}^n e_i^2
$$
接下来，我们需要找到参数向量 $θ=[a_1, a_2, a_3]^T$
 ，使得目标函数 J 最小化。
$$
只使用一阶 \\
\Delta \theta = \theta _k - \theta _{k-1} = - \alpha \nabla J^T(\theta _k) \\
\alpha 为学习率

使用二阶 \\
\Delta \theta = \theta _k - \theta _{k-1} = - \frac {\nabla J^T(\theta _k)} {H(\theta _k)}
$$

接下来就是求 一阶和二阶导数
梯度向量 $J$ 的每个元素是目标函数对每个参数的偏导数：
$$
\nabla J = \begin{bmatrix}
\frac{\partial J}{\partial a_1} \\
\frac{\partial J}{\partial a_2} \\
\frac{\partial J}{\partial a_3}
\end{bmatrix}
$$

对于每个参数 $a_j$，偏导数为：
$$
\frac{\partial J}{\partial a_j} = -2 \sum_{i=1}^n \left(y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)\right) x_i^j
$$

Hessian矩阵 $H$ 的每个元素是目标函数对两个参数的二阶偏导数：
$$
H_{jk} = \frac{\partial^2 J}{\partial a_j \partial a_k}
$$

对于每个参数对 $a_j$ 和 $a_k$，二阶偏导数为：
$$
\frac{\partial^2 J}{\partial a_j \partial a_k} = 2 \sum_{i=1}^n x_i^{j+k}
$$

代入数据计算梯度和Hessian矩阵。
$$
\nabla J = \begin{bmatrix}
-2 \sum_{i=1}^5 \left(y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)\right) x_i \\
-2 \sum_{i=1}^5 \left(y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)\right) x_i^2 \\
-2 \sum_{i=1}^5 \left(y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)\right) x_i^3
\end{bmatrix}
$$

$$
H = 2 \begin{bmatrix}
\sum_{i=1}^5 x_i^2 & \sum_{i=1}^5 x_i^3 & \sum_{i=1}^5 x_i^4 \\
\sum_{i=1}^5 x_i^3 & \sum_{i=1}^5 x_i^4 & \sum_{i=1}^5 x_i^5 \\
\sum_{i=1}^5 x_i^4 & \sum_{i=1}^5 x_i^5 & \sum_{i=1}^5 x_i^6 \\
\end{bmatrix}
$$

代入数据：
- $\sum x_i^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55$
- $\sum x_i^3 = 1^3 + 2^3 + 3^3 + 4^3 + 5^3 = 225$
- $\sum x_i^4 = 1^4 + 2^4 + 3^4 + 4^4 + 5^4 = 979$
- $\sum x_i^5 = 1^5 + 2^5 + 3^5 + 4^5 + 5^5 = 4425$
- $\sum x_i^6 = 1^6 + 2^6 + 3^6 + 4^6 + 5^6 = 19155$

因此：
$$
H = 2 \begin{bmatrix}
55 & 225 & 979 \\
225 & 979 & 4425 \\
979 & 4425 & 19155 \\
\end{bmatrix}
$$

### **步骤 4：迭代更新参数**
1. 选择初始参数 $\theta_0 = [a_1, a_2, a_3]^T$（例如，可以设为零向量）。
2. 计算当前梯度 $\nabla J(\theta_k)$ 和Hessian矩阵 $H(\theta_k)$。
3. 更新参数：
$$
\theta_{k+1} = \theta_k - H^{-1} \nabla J(\theta_k)
$$
4. 重复步骤2和3，直到梯度的范数小于某个阈值（例如，$10^{-6}$）。

### 高斯牛顿法

高斯牛顿法是最优化算法中最简单的方法之一，他的思想是将 f(x) 进行一阶的泰勒展开
$$
f(x + \Delta x) \approx f(x) + J(x) \Delta x
$$

代入求导可得
$$
J(x)^Tf(x) \Delta x = -J(x)^TJ(x)\Delta x
$$

我们要求解的变量是 $\Delta x$, 因此这是一个线性方程组，称为增量方程，也称为高斯牛顿方程或者正规方程
将左边的系数定义为H，右边定义为g，那么方程可以写成
$$
H\Delta x = g \\
\Delta x = \frac {J(x)^Tf(x)} H
$$

可以看出，相比于牛顿法，高斯法使用 $J^TJ$ 替代海森矩阵，省略了计算二阶导数的步骤，因此计算量更小。

那么算法步骤就可以写成

1. 给定初始值 $\theta_0$
2. 重复以下步骤直到收敛
3. 对于第k次迭代，求出当前的雅可比矩阵和误差
4. 求解增量方程，用增量更新参数
5. 若增量小于某个阈值，停止迭代

高斯牛顿法的缺点

1. 要求使用的H矩阵是可逆的，如果不可逆，那么算法无法进行下去
2. 如果H矩阵接近奇异，那么算法的收敛速度会变慢或者无法收敛
3. 由于只使用了一阶导数，所以对于非凸函数，可能会陷入局部最优解

列文伯格-马夸尔特方法在一定程度上修正了这些问题，一般认为比高斯牛顿法更为健壮，尽管他的收敛速度较慢。

### 列文伯格-马夸尔特方法

由于高斯牛顿法中采用的近似二阶泰勒展开只能在展开点附近有较好的近似效果，所以我们自然的想到应该给 $\Delta x$ 添加一个信赖区间 (Trust Region)，即在信赖区间内进行优化，这样可以避免陷入局部最优解。

那么如何确定这个信赖区域的范围呢，一个比较好的方法是根据我们的近似模型跟实际函数之间的差异来确定：

+ 如果差异小，就让范围尽可能的大
+ 如果差异大，就让范围尽可能的小

因此，考虑使用
$$
\rho = \frac{f(\theta_k + \Delta \theta) - f(\theta_k)}{J(x) \Delta \theta}
$$
来衡量信赖区间，
+ 如果 $\rho$ 接近 1，则近似是好的，
+ 如果 $\rho$ 太小，说明实际减少的值越小于预计减少的值，认为近似较差，需要减小信赖区间
+ 如果 $\rho$ 较大，说明实际减少的值大于预计减少的值，认为近似是好的，需要增大信赖区间

因此，列文伯格-马夸尔特方法的步骤可以写成

1. 给定初始值$x_0$ 以及初始优化半径 $\mu$
2. 对于第 k 次迭代，求解：
   $$
    min_{\Delta x} \frac 1 2 || J(x_k) \Delta x + f(x_k) ||^2, s.t.||D \Delta x_k||^2 \le \mu
   $$
    这里 $\mu$ 是信赖区域的半径
3. 计算 $\mu$
4. 若 $\mu \lt \frac 1 4$, 则 $\mu = 2\mu$
5. 若 $\mu \gt \frac 3 4$, 则 $\mu = 0.5\mu$
6. 如果 $\mu$ 大于某阈值，则任务近似可行，令 $x_{k+1} = x_k + \Delta x_k$
7. 重复步骤2-6，直到信赖区域的半径小于某个阈值

这里近似范围扩大的倍数和阈值都是经验值，可以替换成别的数值

我们把增量限定在半径为 $\mu$ 的球中，认为只有在这个球内才是有效的，带上D后，这个球就可以看成一个椭球

我们使用拉格朗日乘子将它转化成一个无约束的优化问题，然后求解
$$
\Delta x = \arg min_{\Delta x} \frac 1 2 || J(x_k) \Delta x + f(x_k) ||^2 + \frac \lambda 2 ||D \Delta x_k||^2
$$
这里 $\lambda$ 为拉格朗日乘子，将其展开后可得

$$
\Delta x = \frac g {H + \lambda I}
$$

可见，当 $\lambda$ 比较小的时候，H矩阵起主要作用，当 $\lambda$ 较大时，D矩阵起主要作用，因此，当信赖区间较大时，我们希望H矩阵起主要作用，当信赖区间较小时，我们希望D矩阵起主要作用，因此，我们可以通过调整 $\lambda$ 来控制信赖区间

#### 实例说明

接下来还是使用上面的那个例子，有一群观测点和观测的结果分别为 x = {1, 2, 3, 4, 5} 和 y = {2, 4, 9, 17, 20}，我们希望找到一个线性模型 $y = a_1x + a_2x^2 + a_3x^3$ 来拟合这些数据

好的，我们可以使用LM（Levenberg-Marquardt）方法来求解这个最小二乘问题。LM方法结合了梯度下降法和高斯牛顿法的优点，适用于非线性最小二乘问题。以下是使用LM方法的步骤：

目标函数为误差平方和：
$$
J(a_1, a_2, a_3) = \sum_{i=1}^n \left(y_i - (a_1x_i + a_2x_i^2 + a_3x_i^3)\right)^2
$$

LM方法通过以下步骤迭代更新参数：
1. **初始化参数**：选择一组初始参数值，这些参数是问题的解的近似值。
2. **计算雅可比矩阵**：对于目标函数，计算其关于参数的雅可比矩阵。
3. **构建正规方程**：利用雅可比矩阵和目标函数的梯度，构建正规方程，并求解该方程组以获得参数的更新值。
4. **更新参数**：将求解得到的参数更新值用于下一次迭代。
5. **检查收敛性**：如果参数更新值足够小，或者迭代次数达到预设的上限，则认为算法已收敛，停止迭代。

##### **步骤 1：初始化参数**

选择初始参数 $\theta_0 = [a_1, a_2, a_3]^T$（例如，可以设为零向量）。在这个例子中
$$
\theta = [0, 0, 0]^T \\
\mu = 0.01
$$

##### **步骤 2：计算雅可比矩阵**

雅可比矩阵 $J$ 的每个元素是模型函数对每个参数的偏导数：
$$
J = \begin{bmatrix}
\frac{\partial f}{\partial a_1} & \frac{\partial f}{\partial a_2} & \frac{\partial f}{\partial a_3}
\end{bmatrix}
$$
对于模型 $f(x, a) = a_1x + a_2x^2 + a_3x^3$，偏导数为：
$$
\frac{\partial f}{\partial a_1} = x, \quad \frac{\partial f}{\partial a_2} = x^2, \quad \frac{\partial f}{\partial a_3} = x^3
$$

##### **步骤 3：构建正规方程**
构建正规方程：
$$
(J^T J + \lambda I) \Delta \theta = J^T r
$$
其中，$r$ 是残差向量，$\lambda$ 是阻尼参数，$I$ 是单位矩阵，$J^TJ$ 是海森矩阵的近似值。

##### **步骤 4：求解正规方程**
求解上述线性方程组，得到参数的增量 $\Delta \theta$。

##### **步骤 5：更新参数**
更新参数：
\[
\theta_{k+1} = \theta_k + \Delta \theta
\]

##### **步骤 6：检查收敛性**
如果参数更新值 $\Delta \theta$ 的范数小于某个阈值（例如，$10^{-6}$），则停止迭代。

##### **步骤 7：调整阻尼参数**
如果目标函数值在更新后减小，则减小阻尼参数 $\lambda$（例如，乘以0.1）；否则，增大阻尼参数 $\lambda$（例如，乘以10）。

##### **步骤 8：重复迭代**
重复步骤2到步骤7，直到满足收敛条件。

## 图优化

### 图优化理论

我们再看一下非线性最小二乘的公式
$$
e_i = y_i - f(x_i, \theta)
$$
最小二乘问题的目标是找到一组参数 \(\theta^*\)，使得误差平方和最小：
$$
\theta^* = \arg\min_{\theta} \sum_{i=1}^n e_i^2 = \arg\min_{\theta} \sum_{i=1}^n (y_i - f(x_i, \theta))^2
$$
代入到SLAM问题内
$$
J(x) = \sum_{k=1}^{N} \sum_{j=1}^{M} e_{y, j, k}^T Q_{k, j}^{-1} e_{y, j, k} + \sum_{k=1}^{N} e_{v, k}^T R_k^{-1} e_{v, k}
$$

仅有一组优化变量和多个误差项，我们并不知道每个误差项的权重，因此，我们引入一个权重矩阵 \(W\)，使得每个误差项的权重可以独立调整:

+ 图优化就是把优化问题表现成图的一种方式，一个图有若干个点和连接这些点的边组成
+ 用定点表示优化变量，用边表示误差项
+ 用边的权重表示误差项的权重

非线性最小二乘问题就对应者一个与之对应的图优化问题，图优化问题的解就是非线性最小二乘问题的解

## 总结

非线性优化不考虑概率问题，仅作为数学问题进行计算，对于一个误差为 e = f(x) 的最小二乘问题，记为$\min_x \frac 1 2 ||f(x)||_2^2$
对于这个求解方法，一种常见的思路就是进行泰勒展开 $f(x+\Delta x) = f(x) + J(x)\Delta x + \frac 1 2 \Delta x^T H \Delta x$

其中 J(x) 是雅可比矩阵，H为海森矩阵

### 最速下降法

如果只保留一阶雅可比矩阵，那么要使误差减少，$g(\Delta x) = J(x)\Delta x$, 就需要 $J(x)\Delta x < 0$，即 $\Delta x = -J(x)^T$ 时，误差下降最快。

### 牛顿法

保留二阶海森矩阵，那么要使误差减少，$g(\Delta) = J(x)\Delta x + \frac 1 2 \Delta x^T H \Delta x$, 对公式求导，另其为0，可以得到
$\Delta x = - \frac {J^T} {H}$

### 高斯牛顿法

在牛顿法的基础上，将其直接作为残差函数并求导，得到 $$\Delta x = - \frac {J^T} {J^T J}$$, 也就是 用 $J^T J$ 代替 $H$

### Levenberg-Marquardt方法

在高斯牛顿法的基础上使用基本不等式，对 $\Delta x$ 添加了一个置信区间，添加一个参数 $\lambda$ 来控制置信区间，使得 $\Delta x$ 的更新更加稳定，公式为
$$\Delta x = - \frac {J^T} {J^T J + \lambda I}$$， 也就是 用 $J^T J + \lambda I$ 代替 $H$

### 使用 Ceres 计算最小二乘问题

```cpp
// 设置代价函数的计算模型
strcut CURVE_FITTING_COST {
  CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) {}
  template <typename T>
  bool operator()(T* abc, T* residual) const {
    residual[0] = T(_y) - ceres::exp(abc[0]*T(_x)*T(x) + abc[1]*T(_x) + abc[2]);
    return true;
  }
  const double _x, _y;
};

double abc[3] = {0, 0, 0};
vector<double> x_data, y_data;
ceres::Problem problem;
for (int i = 0; i < N; ++i) {
  problem.AddResidualBlock(
    new ceres::AutoDiffCostFunction<CURVE_FITTING_COST, 1, 3>(
      new CURVE_FITTING_COST(x_data[i], y_data[i])
    ),
    nullptr,
    abc
  );
}

// 配置求解器
ceres::Solver::options options;
options.linear_solver_type = ceres::DENSE_QR; // 使用稠密QR分解求解线性方程组
options.minimizer_progress_to_stdout = true;

ceres::Solver::Summary summary;
ceres::Solve(options, &problem, &summary); // 求解问题
```

### 使用图优化

1. 定义定点和边的类型
2. 构建图
3. 选择优化算法
4. 调用g2o进行优化

```cpp

```