# 优化器的使用

```python
loss = nn.CrossEntropyLoss()
model = Model()
optim = torch.optim.SGD(model.parameters(), lr=0.01)
for data in dataloader:
  imgs, targets = data
  outputs = model(imgs)
  result_loss = loss(outputs, targets)
  optim.zero_grad() # 清空 模型 的梯度参数， 全部设置为 0
  result_loss.backward()
  optim.step()
```

## 学习速率的调整

```python
import torch.optim.lr_scheduler.StepLR

# 每次在上一步的基础上 称 0.1， 每30次更新一次
scheduler = StepLR(optimizer, step_size = 30, gama=0.1)
```

注意，scheduler 没有实现 zero_grad 方法，但实现了step 方法， 仍要 optim 来进行 zero_grad 步骤
