# 神经结构 卷积层

## 以 2d卷积 为例, 参考 torch.nn.functional 的 conv2d

`torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)->Tensor`

+ input: 输入为一个tensor包括(minibatch, in_channels, iH, iW)
+ weight: 权重 或 卷积核
+ bias: 偏置
+ stride: 步幅, 可以是单个数，也可以是一个元祖（H, W）分别指定横向和纵向的步幅
+ padding: 输入图像的上下左右进行填充， 图像变成(H+2padding, W+2padding),填充数字为0 默认不进行填充

```python
import torch
import torch.nn.functional as F

input=torch.tensor([[1,2,0,3,1],
                    [0,1,2,3,1],
                    [1,2,1,0,0],
                    [5,2,3,1,1],
                    [2,1,0,1,1]])

kernel=torch.tensor([[1,2,1],
                     [0,1,0],
                     [2,1,0]])

input=torch.reshape(input, (1,1,5,5))
kernel=torch.reshape(kernel, (1,1,3,3))

print(input.shape)
print(kernel,shape)

output = F.conv2d(input, kernel, stride=1)
```

`torch.nn.conv2d`

+ in_channels (int): 输入的 channel
+ out_channels (int): 多个卷积核得到多个channel
+ kernel_size (int or tuple): 训练的时候会不断调整，
+ stride
+ padding
+ padding_mode
+ dilation: 默认为1, 卷积核各元素点在 输入 上的间隔（空洞卷积）
+ groups
+ bias

## 实际练习：查看卷积结果

```python
import torch
import torchvision
from torch.nn import Conv2d
from torch.utils.data import dataloader
from torch.utils.tensorboard import SummaryWriter

dataset = torchvision.datasets.CIFAR10("../data", train=False, transform=torchvision.transforms.ToTensor(), download=True)

dataload = torch.utils.data.DataLoader(dataset, batch_size=64)

class Test(torch.nn.Module):
  def __init__(self):
    super(Test, self).__init__()
    self.conv1 = Conv2d(in_channels = 3, out_channels=6, kernel_size=3, stride=1, padding=0)

  def forward(self, x):
    return self.conv1(x)

test = Test()

writer = SummaryWriter("../logs")

step = 0
for data in dataload:
  imgs, targets = data
  output = test(imgs)
  print(imgs.shape)
  print(output.shape)
  writer.add_images("input", imgs, step)
  output = torch.reshape(output, (-1, 3, 30, 30))
  writer.add_images("output", output, step)
  step += 1
```

输入和输出的shape关系
+ Input $(N, C_{in}, H_{in}, W_{in})$
+ Output $(N, C_{out}, H_{out}, W_{out})$
+ $H_{out}=[{{H_{in}+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1} \over stride[0]} + 1]$
+ $W_{out}=[{{W_{in}+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1} \over stride[0]} + 1]$

## 卷积的理解

现在以一个 3x3 的矩阵为例：

```python
[
  [1,2,3],
  [4,5,6],
  [7,8,9],
]

# 卷积核为 2x2
[
  [1,0],
  [0,2],
]

# 首先将输入展开为一维向量
A = [1,2,3,4,5,6,7,8,9]

# 再将卷积核展开并补零,即每一行最前面或者最后补个0，一行的前面或者后面补个0
B = [
  [1,0,0,0,2,0,0,0,0],
  [0,1,0,0,0,2,0,0,0],
  [0,0,0,1,0,0,0,2,0],
  [0,0,0,0,1,0,0,0,2],
]

# 那么卷积的结果就是 (AT表示A的转置)
B * AT = [11,14,20,23]

# 我们再整理一下就是
[
  [11,14],
  [20,23],
]
```

那么转置卷积也就是反卷积呢

```python
# 我们只需要吧 卷积计算转置一下
# 也就是卷积核作为输入操作，输入作为卷积核操作

A = [
  [1,2,3,0,4,5,6,0,7,8,9,0,0,0,0,0],
  [0,1,2,3,0,4,5,6,0,7,8,9,0,0,0,0],
  [0,0,0,0,1,2,3,0,4,5,6,0,7,8,9,0],
  [0,0,0,0,0,1,2,3,0,4,5,6,0,7,8,9],
]

B = [1,0,0,2]

#这样计算 AT*B
AT*B = [1,2,3,0,4,7,10,6,7,16,19,12,0,14,16,18]

# reshape 一下
[
  [1,  2,  3,  0],
  [4,  7, 10,  6],
  [7, 16, 19, 12],
  [0, 14, 16, 18],
]
```
