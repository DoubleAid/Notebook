{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc27902-4a7b-4477-9af3-d42ad4b30274",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "ModelNet40 数据集介绍\n",
    "ModelNet40 是一个广泛用于3D物体识别和分类的点云数据集。它由普林斯顿大学维护，包含40个类别的三维模型，总共有12311个CAD模型。这些模型被分为训练集和测试集，其中训练集包含9843个模型，测试集包含2468个模型\n",
    "\n",
    "在本文中将实现一个 PointNet++ 进行训练和测试\n",
    "\n",
    "## 数据库参考\n",
    "\n",
    "https://www.kaggle.com/datasets/balraj98/modelnet40-princeton-3d-object-dataset\n",
    "\n",
    "这个数据库里有几个文件夹的名称有问题，如果是在线使用的话可能需要在代码里进行判断和修改\n",
    "\n",
    "## 代码实现\n",
    "\n",
    "### 首先是实现 Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 进度条库\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "root_dir = \"/kaggle/input/modelnet40-princeton-3d-object-dataset/ModelNet40\"\n",
    "csv_file = \"/kaggle/input/modelnet40-princeton-3d-object-dataset/metadata_modelnet40.csv\"\n",
    "\n",
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, num_points=2048, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "        self.label_to_idx = { label : idx for idx, label in enumerate(sorted(set(pd.read_csv(csv_file)['class']))) }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 如果传入的参数 idx 是一个张量，就将其转化成数组进行处理\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = self.data_frame.iloc[idx, 1]\n",
    "        path = self.data_frame.iloc[idx, 3]\n",
    "        if label == 'tv':\n",
    "            path = 'tv_stand' + self.data_frame.iloc[idx, 3][2:]\n",
    "        elif label == 'night':\n",
    "            path = 'night_stand' + self.data_frame.iloc[idx, 3][5:]\n",
    "        elif label == 'range':\n",
    "            path = 'range_hood' + self.data_frame.iloc[idx, 3][5:]\n",
    "        elif label == 'glass':\n",
    "            path = 'glass_box' + self.data_frame.iloc[idx, 3][5:]\n",
    "        elif label == 'flower':\n",
    "            path = 'flower_pot' + self.data_frame.iloc[idx, 3][6:]\n",
    "        object_path = os.path.join(self.root_dir, path)\n",
    "        label = self.label_to_idx[label]\n",
    "\n",
    "        points = self.read_off(object_path)\n",
    "\n",
    "        if len(points) < self.num_points:\n",
    "            # 如果点的数量没有达到标准，就进行上采样\n",
    "            print(\"points is too less \", len(points))\n",
    "            indices = np.random.choice(len(points), size=self.num_points, replace=True)\n",
    "            points = points[indices]\n",
    "        elif len(points) > self.num_points:\n",
    "            # 如果点的数量超过了标准，就进行下采样\n",
    "            print(\"points is too large \", len(points))\n",
    "            points = points[np.random.choice(len(points), size=self.num_points, replace=False)]\n",
    "        \n",
    "        points = torch.from_numpy(points).float()\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        if self.transform:\n",
    "            points = self.transform(points)\n",
    "        return points, label\n",
    "\n",
    "    def read_off(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        start_index = 2\n",
    "\n",
    "        if lines[0].strip() != 'OFF':\n",
    "            if lines[0].strip()[0:3] == 'OFF':\n",
    "                remain = lines[0].strip()[3:]\n",
    "                num_vertices = int(remain.split()[0])\n",
    "                start_index = 1\n",
    "            else:\n",
    "                print(file_path)\n",
    "                raise ValueError('Invalid OFF file format')\n",
    "        else:\n",
    "            num_vertices = int(lines[1].split()[0])\n",
    "        vertices = []\n",
    "        for i in range(start_index, start_index + num_vertices):\n",
    "            vertex = list(map(float, lines[i].split()))\n",
    "            vertices.append(vertex)\n",
    "        return np.array(vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfd23a-f251-4d1e-abfe-bcfa9f7eb6d9",
   "metadata": {},
   "source": [
    "### 接下来实现一些辅助的函数\n",
    "\n",
    "主要是最远点采样\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6ce9e-6a9a-464c-af07-88a3377bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- 核心辅助函数 -------------------\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    \n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[torch.arange(B), farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def index_points(points, idx):\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = [1] * len(view_shape)\n",
    "    repeat_shape[1] = points.shape[1]\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    device = xyz.device\n",
    "    B, S, C = new_xyz.shape\n",
    "    _, N, _ = xyz.shape\n",
    "\n",
    "    sqrdists = torch.cdist(new_xyz, xyz)\n",
    "    idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    idx[sqrdists > radius ** 2] = N\n",
    "    idx = idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    idx[idx == N] = 0  # 处理无效索引\n",
    "    \n",
    "    grouped_xyz = index_points(xyz, idx)\n",
    "    grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
    "    return grouped_xyz, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c46db-f942-4dd7-a211-88d47815f639",
   "metadata": {},
   "source": [
    "### 接下来实现PointNet++模块\n",
    "\n",
    "+ 包含3个 SetAbstraction 层，逐步下采样点云并提取特征\n",
    "+ 全局特征经过3个券链接层进行分类\n",
    "+ 使用 BatchNorm 和 Dropout 提升泛化能力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1496e5-3e44-4ffd-9c6e-2826eda7c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all=False):\n",
    "        super().__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.group_all = group_all\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        \n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz = index_points(xyz, farthest_point_sample(xyz, self.npoint))\n",
    "            grouped_xyz, grouped_points = query_ball_point(\n",
    "                self.radius, self.nsample, xyz, new_xyz)\n",
    "            \n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, idx)\n",
    "                grouped_points = torch.cat([grouped_xyz, grouped_points], dim=-1)\n",
    "            else:\n",
    "                grouped_points = grouped_xyz\n",
    "\n",
    "        grouped_points = grouped_points.permute(0, 3, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            grouped_points = F.relu(bn(conv(grouped_points)))\n",
    "        \n",
    "        new_points = torch.max(grouped_points, 2)[0]\n",
    "        return new_xyz, new_points\n",
    "\n",
    "# ------------------- 完整模型 -------------------\n",
    "class PointNet2Cls(nn.Module):\n",
    "    def __init__(self, num_classes=40):\n",
    "        super().__init__()\n",
    "        self.sa1 = PointNetSetAbstraction(\n",
    "            512, 0.2, 32, 3, [64, 64, 128], False)\n",
    "        self.sa2 = PointNetSetAbstraction(\n",
    "            128, 0.4, 64, 128+3, [128, 128, 256], False)\n",
    "        self.sa3 = PointNetSetAbstraction(\n",
    "            None, None, None, 256+3, [256, 512, 1024], True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B, _, _ = xyz.size()\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        \n",
    "        l1_xyz, l1_points = self.sa1(xyz, None)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        \n",
    "        x = l3_points.view(B, 1024)\n",
    "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f10f7-7b1c-4cad-a73d-cf02a469a79c",
   "metadata": {},
   "source": [
    "### 接下来添加测试和训练的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d32c42-e5a5-4b09-9ecd-b83be23bcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        data = data.to(device).permute(0, 2, 1)  # [B, 3, N]\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "# 测试函数\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(test_loader):\n",
    "            data = data.to(device).permute(0, 2, 1)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss/len(test_loader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab046fb-0b88-4110-b20d-660c67c295c8",
   "metadata": {},
   "source": [
    "### 主程序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8a772-a24c-4e6e-afed-93287275238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 初始化\n",
    "    dataset = ModelNet40Dataset(csv_file=csv_file, root_dir=root_dir, num_points=8192)\n",
    "\n",
    "    train_indices = dataset.data_frame[dataset.data_frame['split'] == 'train'].index\n",
    "    test_indices = dataset.data_frame[dataset.data_frame['split'] == 'test'].index\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)    \n",
    "    \n",
    "    model = PointNet2Cls().to(config['device'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, config['device'])\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, config['device'])\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]}')\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f'Best Test Accuracy: {best_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
