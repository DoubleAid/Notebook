# Loss
Loss 计算实际输出和目标之间的差距，并作为反向传播的依据
loss 越小越好


+ nn.L1Loss: 输出和目标各元素的差的绝对值的平均值
  ```
  out = 1, 2, 3
  target = 1, 2, 5
  L1Loss = (0+0+2)/3 
  ```
  + reduction 可以使 `mean` 或者 `sum`

+ MSELoss: 平方差
  ```
  out = 1, 2, 3
  target = 1, 2, 5
  L1Loss = (0+0+4)/3 
  ```

+ nn.CrossEntropy: 交叉熵
  主要是用于分类问题,损失函数的定义如下  
  $loss(x, class) = -log({{e^{x_{class}}} \over {\sum_je^{x_j}}})=-x_{class} + log(\sum_je^{x_j})$

  以一个图片的三分类为例：
  一张苹果的图片， 经过神经网络的预测， 苹果的概率是 0.3， 橘子的概率是 0.2， 香蕉的概率是 0.1

  那么损失函数为 $loss(x, class)=-0.3+log(exp(0.1)+exp(0.2)+exp(0.3))$

  ```
  x = torch.tensor([0.1, 0.2, 0.3])
  y = torch.tensor([2])
  # reshape 1 条输入， 3 个类别 即 一行三列
  x = torch.reshape(x, (1, 3))
  loss = nn.CrossEntropyLoss()
  result_loss = loss_cross(x, y)
  print(result_loss)
  ```

## 反向传播， 使用 loss 的 backward
  ```
  loss.backward()
  ```