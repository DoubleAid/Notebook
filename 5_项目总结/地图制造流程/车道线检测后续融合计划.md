# 差到西安检测后续融合计划

## 1. 后续融合流程
当前实现的车道线检测后续融合的流程如 图1 所示

![图1 现阶段后续融合流程](https://cdn.nlark.com/yuque/0/2022/png/25509031/1669023180257-a532a692-8b75-443b-9607-e721897873a7.png)
bag 采集的数据 topic 包括  

+ /current_pva、
+ /hesai_64_points
+ /pylon_sweepfront/image_raw、
+ /pylon_sweepfrontleft/image_raw、
+ /pylon_sweepfrontright/image_raw

车道线提取流程：

+ 获取三个camera topic的 image数据，通过模型提取出相应的车道线；
+ 并通过图片上的横向灰度对比，对识别的车道线优化，并通过点云数据在图片上的投影确定交点在点云中的3D位置信息。 
+ 通过对比识别的车道线的向量信息，交点的位置信息， 将交点进行分类存储，存储的数据会转化成地图坐标下的点， 但也会保存点的高度数据。
+ 在将整个 bag 的数据记录完成之后， 会读取存储的数据再次进行聚类， 消除一条车道线多次检测出现的多条检测线的问题， 再对拟合后的数据分小段重新采样并进行2d平滑处理， 最后保存数据， 根据需要转化成 geojson 文件输出

## 2. 当前进展和问题

当前已经完成了 /pylon_sweepfront/image_raw 中车道线提取到输出的所有流程。

+ 上周已经添加了/pylon_sweepfrontleft/image_raw 和 /pylon_sweepfrontright/image_raw， 
+ 修改了 车道线提取流程， 对原来的车道线点位置纠正根据车道线的方向和 camera 的配置文件的 tf_vehicle_camera 数据重新进行了调整， 尝试相同时间下左右侧检测的车道线能否和前方检测的车道线数据进行拟合成一个车道线数据进行存储。整体上 侧方车道线提取 和 之前的 sweepfront/image_raw 相差不大
+ 红绿灯识别上周只添加了模型， 还没有完成红绿灯识别的过程。

本周

+ 完成两侧车道线的提取的整个流程，提高识别的精度， 尝试通过侧方车道线的检测提高原来车道线的精度
+ 添加 红绿灯位置的检测， 通过连续的多张检测结果， 求出位置的最优解
+ 数据导出方面添加高度导出的方法， 修改原来的地图对比方法



问题：

+ 现在使用的是2d的平滑算法， 后续可以转化成3d的平滑算法。
+ 现在程序存储的数据是放到本地的数据库中的， 在做最后的聚类时需要读取大量的数据， 这部分需要再优化一下
+ 现在的检测模型在转弯掉头时的误差很大， 掉头时的车道线检测结果不好



方案注意事项：

1.车道线

  1）用精细化标定的结果，M002有100c，精度更高

  1) 只用画面最下面1/3的检测结果

  3）线拟合 ，不做平滑

  4）定位结果用inspva，判断inspvax里面的std，std过大的结果不用

  5）先用正前方的camera，可以看看后方的camera

  6）提取结果，精度要求，20cm

  7）基于提取结果，检测道路车道线变化区域，提醒地图进行更新

2.红绿灯

  1）用感知的模型，2D框，通过大小计算距离

      红绿灯设定固定的大小，1.5m * 0.6m ？

      设定先验值表格，width or height 对应的，距离

      距离越大，2D框越小，计算出大致距离，精度要求50cm （暂定）

  2）输出geojson结果

     包括坐标，width，height，还能不能输出红绿灯的类型（箭头还是圆灯），分布类型（横向红黄绿还是竖向红黄绿），黄闪看看能不能识别

     灯不亮的时候，结果是怎么样

  3）基于提取结果，和地图对比，检测道路红绿灯变化区域，提醒地图进行更新
