# <font color="skyblue">后端1</font>

## <font color="deepskyblue">主要目标</font>

1. 理解后端的概念
2. 理解以EKF为代表的滤波器后端工作原理
3. 理解非线性优化的后端，明白稀疏性是如何利用的
4. 使用g2o和Ceres实际操作后端优化

前端里程计只能给出一个短期的轨迹和地图, 由于不可避免的误差累积，这个地图在长时间内是不准确的。所以在视觉里程计的基础上，还希望构建一个尺度规模更大的优化问题，考虑长时间的最优轨迹和地图

## <font color="deepskyblue">概述</font>

1. 由于SLAM特征点众多，观测方程的数量远大于运动方程
2. 没有运动方程时，问题转化成SfM问题
3. 噪音符合高斯分布

根据上面的条件，问题就转变成：存在一些运动数据和观测数据，如何去估计状态量的高斯分布

假设我们作为机器人蒙着眼睛行走，刚开始我们还是可以确定自己的方向和位置，随着移动，越来越不确定自己的位置和方向

+ 虽然我们每一步的长度自己都是可以估计出大概的，但随着时间的增长，我们对自己的位置就越来越不确定，输入受噪音的影响，位置估计的方差越来越大
+ 如果我们看得到周围的场景，就可以不断修订自己步伐的误差

将位姿和路标点所有待估计的变量计作x_k, 那么运动方程就变成了
$$
X_k = {x_k, y_1, y_2, y_3,...,y_m} \\
X_k = f(X_{k-1}, u_k) + w_k \\
z_k = h(X_k) + v_k
$$

如果现在我们希望用过去0到k中的数据来估计现在的状态分布
$$
P(x_k | x_0, u_{1:k}, z_{1:k})
$$

通俗理解就是蒙眼走路时知道起始位置，每一步的方向和步长，一个路标点的位置，那么我们就可以估计出当前位置的可能性

根据贝叶斯法则我们可以得出

$$
P(x_k | x_0, u_{1:k}, z_{1:k}) \propto P(z_k | x_k)P(x_k | x_{k-1}, u_{1:k-1})
$$

+ $P(z_k | x_k)$ 也就是知道当前位置下路标点的观测值, 称为似然，由我们现在的眼睛观测得到，也就是观测方程得出的
+ $P(x_k | x_{k-1}, u_{1:k-1})$以及上一步的状态分布，称为先验，由过去所有的状态估计出来的，也就是运动方程得出的

就可以估计出当前位置的分布

$$
P(x_k | x_{k-1}, u_{1:k-1}) = P(x_{k-1}) + u_kdx
= \int P(x_k | x_{k-1}, u_k)P(x_{k-1})dx_{k-1}
$$

也就是当前位置的分布是上一步的分布加上当前的运动, 这样我们就只关心k的k-1时刻的情况，

接下来的处理分成了两部分
+ 假设马尔可夫性，即简单的一阶马氏性认为k时刻的状态只和k-1的状态有关，与之前的运动无关，这样就得到了以扩展卡尔曼滤波（EKF）为代表的滤波器
+ 另一种方法是考虑k之前的所有状态的关系，也就是讲 $P(x_{k-1})$进一步分解，此刻得到非线性优化为主体的优化框架，目前主流的是非线性优化方法

## <font color="deepskyblue">线性系统和卡尔曼滤波</font>

马尔可夫性（Markov Property）是概率论和随机过程中的核心概念，描述了一种“无记忆”特性，即​​未来的状态仅依赖于当前状态，而与过去的历史状态无关​​。

在这个假设下
$$
P(x_k | x_0, u_{1:k}, z_{1:k}) = P(x_k | x_{k-1}, u_k)
$$

由于$u_k$是有噪音的移动向量，噪音与过去无关，所以 k-1 时刻的位置和步长无关
于是这个问题就变成了如何从k-1时刻的状态分布推导出k时刻的分布，如果k-1时刻的分布是符合某一个位置的高斯分布，那么，我们只需要更新位置的均值和协方差就可以了

$$
x_k = A_kx_{k-1} + B_ku_k + w_k \\
z_k = C_kx_k + v_k \\
$$

假设噪音符合高斯分布
$$
w_k ～ N(0, Q_k) \\
v_k ～ N(0, R_k)
$$

假设 $x_k$ 和 $z_k$ 都是高斯分布，那么 $x_k$ 的均值$E_k$和协方差$D_k$就可以通过 $x_{k-1}$ 的均值$E_{k-1}$
和协方差D_{k-1}推导出来

$$
P(x_k | x_{k-1},u_k) = N(A_kE_{k-1}^{后验} + B_ku_k, A_kD_{k-1}^{后验}A_k^T + Q_k)
$$

也就是当前位置的高斯分布应该符合以这样的一个高斯分布，我们将当前的位姿的估计协方差称为 $D_{先验}$，均值称为$E_{先验}$

接下来计算当前位置观测到观测点的高斯分布，因为我们已经知道了当前位置的高斯分布，代入到公式中就可以得到
$$
P(z_k | x_k) = N(C_kE_{先验}, C_kD_{先验}C_k^T + R_k)
$$

那么，当前位置下的分布就可以通过贝叶斯法则计算出来
$$
N(E_{后验}， D_{后验}) = N(C_kE_{先验}, C_kD_{先验}C_k^T + R_k) * N(E_{先验}, D_{先验})
$$

我们求高斯分布的指数部分，也就是$\frac {(x - Ex)^2} {Dx}$

$$
\frac {(x-E_{后验})^T(x-E_{后验})} {D_{后验}} = \frac {(z_k - C_kE_{先验})^T(z_k-C_kE_{先验})} {C_kD_{先验}C_k^T + R_k} + \frac {(x - E_{先验})^T(x_k - E_{先验})} {D_{先验}}
$$

展开后比较一次项和二次项的系数，可以得到
$$
E_{先验} = A_kE_{k-1}^{后验} + B_ku_k \\
D_{先验} = A_kD^{后验}_{k-1}A_k^T + Q_k \\
取 K = D_{先验}C_k^T(C_kD_{先验}C_k^T + R)^{-1} = \frac {D_{先验}C_k^T} {C_kD_{先验}C_k^T + R} \\
E_{后验} = E_{先验} + K(z_k - C_kE_{先验}) \\
D_{后验} = (I - K C_k)D_{先验} = D_{先验} - K C_kD_{先验}
$$

这就是<font color="red">卡尔曼滤波的迭代</font>

## <font color="skyblue">非线性系统和EKF</font>

首先要澄清一点，SLAM的运动方程和观测方程通常是非线性函数，因此，直接应用卡尔曼滤波器是不合适的。然而，扩展卡尔曼滤波器（EKF）通过线性化非线性函数来近似非线性系统的行为，从而在一定程度上解决了这个问题。

我们希望把卡尔曼滤波的结果拓展到非线性系统中，称为扩展卡尔曼滤波，通胀的做法是在某个点的附近进行泰勒展开，只保留一阶项，然后用线性卡尔曼滤波器来处理这个线性化后的系统

我们将运动方程和观测方程在 $x = E_{后验，k-1}$ 处展开，
$$
x_k = f(E_{后验 k-1}, u_k) + \frac {\partial f}{\partial x_{k-1}} {\mid}_{E_{后验 k-1}}(x_{k-1} - E_{后验 k-1}) + w_k \\
$$

其中 $\frac {\partial f}{\partial x_{k-1}}$ 是运动方程对 $x_{k-1}$ 的雅可比矩阵记为F，$w_k$ 是运动噪声，$u_k$ 是控制输入

同样的观测方程就变成了
$$
z_k = h(E_{先验}) + \frac {\partial h} {\partial x_k} {\mid}_{E_{先验}}(x_k - E_{先验}) + v_k \\
$$

其中 $\frac {\partial h} {\partial x_k}$ 是观测方程对 $x_k$ 的雅可比矩阵记为H，$v_k$ 是观测噪声，$z_k$ 是观测值，对于先验的均值，因为代入的是k-1处的后验数据 $E(F(x_{k-1} - E_{后验,k-1})) = 0$
那么 $P(x_k | x_{k-1},u_k)$ 就变成了
$$
P(x_k | x_{k-1},u_k) = N(f(E_{后验，k-1}, u_k), FD_{后验,k-1}F^T + Q_k) \\
记 E_{先验} = f(E_{后验，k-1}, u_k) \\
D_{先验} = FD_{后验,k-1}F^T + Q_k \\
P(z_k | x_k) = N(h(E_{先验}) + H(x_k-E_{先验}), HD_{先验}H^T + R_k) \\
$$
最后可以得到
$$
协方差预测（也就是离散程度预测）\\
D_{先验} = FD_{后验,k-1}F^T + Q_k \\
状态预测（也就是均值预测）\\
E_{先验} = f(E_{后验，k-1}, u_k) \\
卡尔曼增益\\
 K_k = D_{先验}H^T(HD_{先验}H^T + R_k)^{-1} \\
状态更新（也就是均值更新）\\
E_{后验} = E_{先验} + K_k(z_k - h(E_{先验})) \\
协方差更新（也就是离散程度更新）\\
D_{后验} = (I - K_k H)D_{先验} \\
$$

它具有以下缺陷

+ ​​一阶近似误差​​：高阶泰勒项被忽略，可能导致状态估计偏差。
+ ​雅可比矩阵计算复杂度​​：需实时计算非线性函数的偏导数，对复杂模型计算量大。
+ ​鲁棒性不足​​：在高度非线性或非高斯噪声场景下性能下降。

## <font color="skyblue">BA和图优化</font>

BA（Bundle Adjustment）是一种优化方法，用于解决SLAM中的重定位和地图优化问题。它通过最小化重投影误差来优化相机位姿和地图点的位置。

在图优化框架的视觉SLAM算法里，BA起到了核心作用。他类似于求解只有观测方程的SLAM问题，在最近几年视觉SLAM理论中，BA具有良好的精度和实时性

1. 当我们把世界坐标系下的一个点投影到相机坐标系下，这里会用到相机的外参
   $$
   P_c = RP_w + t = [X^,,Y^,,Z^,]
   $$
2. 然后将P_c投影到归一化坐标系下
   $$
    P_g = [u_g, v_g, 1]^T = [X^,/Z^,1]
   $$
3. 考虑归一化坐标的畸变情况，得到去畸变前的原始像素坐标，这里暂时只考虑径向畸变
   $$
    u_c^, = u_g(1 + k_1r_c^2 + k_2r_c^4) \\
    v_c^, = v_g(1 + k_1r_c^2 + k_2r_c^4) \\
   $$
4. 最后根据内参模型，计算出像素坐标
   $$
   u_s = f_xu_c^, + c_x \\
   v_s = f_yv_c^, + c_y \\
   $$

这一系列计算流程就是之前提到的观测方程 $z = h(x, y)$

这里面 x 代指相机位资，也就是 R 和 t，我们将其对应的李代数记为 $\zeta$ y 代指地图点的位置，也就是 X, Y, Z, 我们将观测数据的像素坐标为 [u_s, v_s],我们将这个坐标记为z
那么误差就变成了
$$
e = z - h(\zeta, p)
$$

这里 p 是路标的真实位置，那么代价函数就变成了
$$
\frac 1 2 \sum_{i=1}^N ||e_i||^2 = \frac 1 2 \sum_{i=1}^N ||z_i - h(\zeta_i, p_i)||^2
$$

这个就变成了一个最小二乘进行求解，相当于对位资和路标进行优化，也就是所谓的BA

然后使用牛顿法对这个代价函数进行求解
我们把所有的自变量都记为 x，那么
$$
x = [\zeta_1, \zeta_2, ..., \zeta_m, p_1, p_2, ..., p_n]^T \\
e = z - h(x)
$$


为了使误差最小，我们对自变量做一些修正，记 $x = x + \Delta x$ 那么目标函数就变成了
$$
\frac 1 2 ||f(x + \Delta x)||^2 = \frac 1 2 ||f(x) + J\Delta x||^2 \\
 = \frac 1 2 \sum_{i=1}^m \sum_{j=1}^n ||e + F\Delta \zeta + E\Delta p||^2
$$

其中 F 是针对相机位姿的雅可比矩阵，E 是针对路标位置的雅可比矩阵

无论我们使用哪种优化方法，最后都会面对增量线性方程
$$
H\Delta x = g
$$

我们知道高斯牛顿法和列文伯格-马夸尔特方法的区别就在于 $H = J^TJ$还是$H = J^J + \lambda I$ 的形式，由于我们吧变量分为了位姿部分和空间部分，整个雅可比矩阵就可以分为 $J = [F, E]$

那么以高斯牛顿法为例，海森矩阵就变成了
$$
H = J^TJ = \begin{bmatrix}
F^TF & F^TE \\
E^TF & E^TE
\end{bmatrix}
$$

可以发现这个线性方程的规模非常大，他包括历史的所有位姿和场景内的所有路标，因此，我们不能直接求解这个线性方程，而是采用稀疏求解的方法，也就是只求解非零元素，这样可以大大降低计算量

### <font color="YellowGreen">稀疏性和边缘化</font>

21世纪SLAM的一个重要进展就是认识到 H 的稀疏性，并发现该参数可以自然，显式的使用图优化来表示
我们现在考虑在 位姿 i的时候看到路标 j 时的场景，那么这个场景可以表示为
$$
J_{ij}(x) = (0_{2x6}, ..., 0_{2x6}, \frac {\partial e_{ij}} {\partial \zeta_i}, 0_{2x6}, ..., 0_{2x3}, \frac {\partial e_{ij}} {\partial p_j}, 0_{2x3}, ..., 0_{2x3})
$$

这样的误差矩阵的雅可比矩阵除了这两处，其他都是为零，这体现了该误差项和其他路标和轨迹无关
那么 对于海森矩阵 $H = J_{ij}^TJ_{ij}$ 这个矩阵也只有四处为0，分别是 (i,i), (i,j), (j,i), (j,j) 这四个位置，其他位置都是非零的，这就是所谓的稀疏性

那么整个四个部分，
$$
H = \begin{bmatrix}
H_{mm} & H_{nm} \\
H_{mn} & H_{nn}
\end{bmatrix}
$$

可以看出来，$H_{mm}$ 和 $H_{nn}$ 一定是一个对角阵
对于 $H_{mn}$和$H_{nm}$ 如果我每个位置只能看到很少的路标点，那么就是稀疏的，如果观测的数据很多，那么就是稠密的，根据观测数据而定，而且两个矩阵是互为转置的，所以 $H_{mn} = H_{nm}^T$

对于这样的矩阵，一种常用的手段是 Schur消元，在SLAM中也称为边缘化

我们再简化一下，将 $H_{mm} = B$, $H_{nn} = C, H_{nm} = E, H_{mn} = E^T$

那么增量方程就变成了
$$
\begin{bmatrix}
B & E \\
E^T & C
\end{bmatrix}\begin{bmatrix}
\Delta \zeta \\
\Delta p
\end{bmatrix} = \begin{bmatrix}
v \\
w
\end{bmatrix}
$$

我们计划消掉右上的E
$$
\begin{bmatrix}
I & -EC^{-1} \\
0 & I
\end{bmatrix}\begin{bmatrix}
B & E \\
E^T & C
\end{bmatrix}\begin{bmatrix}
\Delta \zeta \\
\Delta p
\end{bmatrix} = \begin{bmatrix}
I & -EC^{-1} \\
0 & I
\end{bmatrix}\begin{bmatrix}
v \\
w
\end{bmatrix}
$$
整理一下可以得到
$$
\begin{bmatrix}
B - EC^{-1}E^T & 0 \\
E^T & C
\end{bmatrix}\begin{bmatrix}
\Delta \zeta \\
\Delta p
\end{bmatrix} = \begin{bmatrix}
v - EC^{-1}w \\
w
\end{bmatrix}
$$

可以看出，第一行已经和观测点没有关系了
$$
[B -EC^-1E^T]\Delta \zeta = v - EC^-1w
$$

所以可以先求解出这个矩阵，得到 $\Delta \zeta$, 之后再求解 $\Delta p$，这就是所谓的边缘化或者称为消元

它有以下优势

1. 在消元过程中，由于C为对角块，所以求解过程非常简单，只需要对角线上的元素求逆即可。
2. 求解出 $\Delta \zeta$ 后，可以将 $\Delta \zeta$ 代入到 $H_{mm}$ 中，得到新的 $H_{mm}$，这样可以减少海森矩阵的规模，从而降低计算量。

边缘化的主要计算量就在于计算出 $\Delta \zeta$, 而我们将 $[B -EC^-1E^T]$ 记为S
可以看出，S矩阵的非对角线上的非零矩阵块，表示了该处对应的两个相机变量之间存在着共同观测的路标点，称为共视，如果该块为零，表示两个相机变量之间没有共同观测的路标点，称为不共视

所以，S阵的稀疏性结构还是取决于实际观测的结果，在实践中，如在ORB-SLAM local mapping 中，会可以选择具有共同观测的帧作为关键帧，得到的S矩阵是稠密矩阵

在 DSO，OKVIS等，采用滑动窗口方法，这类要对每一帧都做一次BA防止误差的累积，因此也必须采取一些技巧保持S阵的稀疏性

从概率上讲，称这一步为边缘化的原因是将求 $(\Delta \zeta, \Delta p)$的问题转化成先求位姿$\Delta \zeta$, 后求 $\Delta p$

$$
P(\Delta \zeta, \Delta p) = P(\Delta p | \Delta \zeta)P(\Delta \zeta)
$$

结果就是求位姿的边缘分布，所以称为边缘化

### <font color="YellowGreen">鲁棒性函数</font>

在实际使用中，会存在误匹配的情况，这样就把原本不存在的边，加到了数据中了，优化算法并不能区分出这是一条错误数据，当误差很大时，二范数增长的很快，导致优化算法对这条错误数据非常敏感，这就是所谓的鲁棒性问题

具体的做法就是，把原来误差的二范数度量替换成一个增长不快的函数，同时保证自己光滑的性质，使整个优化结果更加稳健，称为鲁棒核函数

常用的鲁棒核函数有 Huber 函数和 Tukey 函数

$$
Huber： H(e) = \begin{cases}
\frac 1 2 e^2 & |e| \leq \delta \\
\delta |e| - \frac 1 2 \delta^2 & |e| > \delta
\end{cases}
$$

当误差大于某个阈值后，函数增长由而二次变成了一次，相当于限制了梯度的最大值
