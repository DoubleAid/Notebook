
# 后端4 因子图优化

图优化是将 SLAM 系统的状态估计问题转化为图的最小二乘优化问题，用节点（Nodes） 表示待优化变量（如每帧位姿 $T_0, T_1, ..., T_n$），用边（Edges） 表示变量间的约束（如帧间相对位姿约束、回环约束）。每个边对应一个误差项 $e_i$，优化目标是最小化所有误差项的加权和

在SLAM中的顶点有：

+ 核心顶点：各帧的位姿（SE (3)，包括平移和旋转），3D 地图点坐标（视觉 SLAM）
+ 扩展顶点：IMU 零偏（多传感器融合 SLAM）、相机内参（需在线标定场景）；

## 图优化的数学框架

$$
\min_x∑_{i=1}^mw_i⋅\rho(e_i(x))
$$

+ x: 所有节点组成的变量向量（如位姿的平移 + 旋转参数）；
+ m：约束边的数量；
+ $w_i$：第 i 条边的权重（待更新）；
+ $\rho(e_i)$: 损失函数（L2/Huber/Cauchy）；
+ $e_i(x)$: 第 i 条边的误差（如点云配准的点到面距离、位姿约束的偏差）。

## 常见的误差项

## 损失函数 / 鲁棒核

### 1. L2 损失（平方误差）

$$
\rho(r) = r^2
$$

#### L2 优点

+ 数学性质友好：损失函数是凸函数、连续可导，优化时（如高斯 - 牛顿法、梯度下降）收敛稳定，易工程实现；
+ 对小误差敏感：能有效惩罚微小偏差，适合 “无异常值、数据噪声服从高斯分布” 的理想场景。

#### L2 缺点

+ 对异常值极不鲁棒：异常值的误差 \(\epsilon\) 通常很大，平方后会被放大数倍，主导整个损失函数，导致优化结果向异常值偏移（例如 ICP 配准中，1 个误匹配点的误差可能掩盖 100 个正确匹配点的贡献）；
+ 假设限制严格：仅适用于噪声服从高斯分布的场景，而 SLAM 中实际噪声（如动态物体、激光遮挡）多为非高斯分布。

### 2. Huber 核函数

Huber 损失是为解决 L2 损失对异常值敏感的问题而提出的分段损失函数，核心是：

+ 当误差 \(|\epsilon| \leq \delta\)（\(\delta\) 为阈值，通常取 1.0~2.0）时，用 L2 损失（惩罚小误差，保证优化精度）
+ 当误差 \(|\epsilon| > \delta\) 时，用 L1 损失（线性惩罚大误差，抑制异常值影响）

$$
\rho(r) = \begin{cases} 
\frac{1}{2} r^2 & \text{if } |r| \leq \delta \\
\delta \left(|r| - \frac{1}{2} \delta \right) & \text{if } |r| > \delta 
\end{cases}
$$

#### Huber 优点

+ 鲁棒性强：对异常值的惩罚是线性的，不会被平方放大，有效避免优化结果偏移；、
+ 兼顾精度与鲁棒性：小误差时保留L2损失的凸性和高精度，大误差时切换为L1的鲁棒性；
+ 阈值可调：通过 ( \delta ) 控制“鲁棒程度”——( \delta ) 越小，对异常值越敏感（更接近L1）；( \delta ) 越大，越接近L2。

#### Huber 缺点

+ 需手动调参：( \delta ) 的选择依赖场景（如激光点云配准中，( \delta ) 通常设为点云噪声的2~3倍）；
+ 导数不连续：在 ( |\epsilon| = \delta ) 处导数突变，优化时需特殊处理（但工程中影响较小）

### 3. Cauchy损失：对异常值“极度宽容”的鲁棒损失

Cauchy损失（又称洛伦兹损失）是基于Cauchy分布的损失函数，核心是对大误差的惩罚增长极其缓慢（远慢于L2和Huber），数学形式为：

$$
\rho(r)=δ^2ln(1+(\frac r δ)^2)
$$

其中 ( \delta ) 为尺度参数（类似Huber的阈值，通常取1.0~3.0）。

#### 优点

+ 鲁棒性极强：大误差的惩罚呈对数增长，即使存在大量异常值（如30%以上的误匹配点），也不会主导损失函数，优化结果依然稳定；
+ 无导数不连续问题：全区间可导，优化时收敛平滑。

#### 缺点

+ 小误差惩罚不足：对微小偏差的敏感度低于L2和Huber，可能导致优化精度略低（“过度鲁棒”）；
+ 收敛速度慢：损失函数的梯度随误差增大衰减快，优化后期梯度变小，收敛速度比L2慢。

### 比较

+ 平方误差核（L2）：仅适用于无异常值的理想场景（如室内静态环境）；
+ Huber 核：兼顾精度和鲁棒性，适用于异常值较少的场景（如轻度动态、小雨天气）；
+ Cauchy 核：鲁棒性最强，适用于异常值较多的场景（如重度动态、大雨 / 沙尘天气）。

## 权重更新

图优化中，权重 $w_i$ 并非独立设置，而是由损失函数 $\rho(r_i)$ 的二阶导数（Hessian） 决定 ——— 因为优化问题最终会转化为线性方程组 $\mathbf{H}\Delta\mathbf{x} = \mathbf{b}$，其中 $\mathbf{H}$（Hessian 矩阵）和 $\mathbf{b}$（梯度向量）的构建依赖损失函数的导数，而权重本质是损失函数对误差的 “惩罚强度” 量化。

对于任意损失函数 $\rho(r)$, 定义权重因子 $w(r)$：

$$
w(r) = \frac {d^2\rho(r)} {dr^2}
$$

（物理意义：损失函数在误差 e 处的曲率，曲率越大，对该误差的惩罚越强，权重越大）

### L2 损失（平方误差）的权重更新

$$
w_{\rho2} = \frac {d^2\rho(r)} {dr^2} = 1
$$

+ L2 损失的权重因子 $w_{L2}(r) = 1$（常数），意味着：所有约束边的权重固定为 1，不随误差大小变化。
+ 物理意义：无论误差是小（有效约束）还是大（异常值），都给予相同的惩罚强度 —— 这也是 L2 对异常值敏感的根本原因（大误差的平方项会主导优化）。
+ 工程实现：无需动态更新权重，所有边的权重初始化为 1，优化过程中保持不变。

### Huber 损失的权重更新

+ 当 $|r| < \delta$ (小误差)
  $$
  \frac {d\rho} {dr} = r \\
  w_{huber(r)} = 1
  $$
+ 当 $|r| > \delta$ (大误差)
  $$
  \frac {d\rho} {dr} = \delta |r|
  w_{huber(r)} = \frac {\delta} {r}
  $$

总结一下

$$
w_{huber(r)} = \frac {\delta} {max(|r|, \delta)}
$$

+ 当 $|e| \leq \delta$：$w_{\text{Huber}}(e) = 1$（权重 = 1，和 L2 一样惩罚小误差）；
+ 当 $|e| > \delta$：$w_{\text{Huber}}(e) = \frac \delta {|e|}$（误差越大，权重越小，惩罚强度线性衰减）。
+ 物理意义：对小误差（有效约束）保持高精度惩罚，对大误差（异常值）降低权重，避免其主导优化。

### Cauchy 损失的权重更新

$$
\frac {d\rho} {de} = \frac {2\delta e} {\delta^2 + e^2} \\
w_{cauchy(e)} = \frac {\delta^2} {\delta^2 + e^2}
$$

+ 当 $|e| \approx 0$（小误差）：$\rho_{\text{Cauchy}}(e) \approx 1$（权重≈1，惩罚小误差）；
+ 当 $|e|$ 增大（大误差）：$\rho_{\text{Cauchy}}(e)$ 随 $e^2$ 增长而衰减，且衰减速度远快于 Huber（对数级衰减）；
+ 当 $|e| \gg \delta$（极大误差）：$\rho_{\text{Cauchy}}(e) \approx \frac {\delta^2} {e^2}$（权重趋近于 0，几乎不惩罚异常值）。
+ 物理意义：对异常值的惩罚强度衰减极快，即使存在大量大误差约束，也不会影响优化结果 —— 鲁棒性最强。
