# 激活函数介绍

激活函数是深度学习模型的 “核心灵魂”—— 它为线性的卷积 / 全连接层引入非线性，让模型能拟合复杂的非线性关系（比如车道线的弯曲、不规则形状）。

下面按「常用程度 + 适配场景」梳理主流激活函数，结合你的机器人感知 / 车道线识别需求，讲清每个函数的定义、优缺点、适用场景：

## 激活函数的核心作用

先明确为什么需要激活函数：

卷积 / 全连接层的计算本质是「线性变换」（y=Wx+b），多层线性变换叠加后仍为线性，无法拟合车道线分割、点云聚类等非线性任务；激活函数通过非线性映射，让模型具备学习复杂模式的能力。

## 常见激活函数详解

### ReLU（Rectified Linear Unit）

**定义**

ReLU(x)=max(0,x)

通俗说：小于 0 的数输出 0，大于 0 的数原样输出（“单侧抑制”）。

**优点**

+ 计算极快：仅需比较和取最大值，GPU 并行效率高；
+ 缓解梯度消失：正区间梯度恒为 1，深层网络训练时梯度不易衰减（U-Net 编码器 / 解码器的核心激活函数）；
+ 稀疏性：约 50% 的神经元输出 0，减少冗余计算，降低过拟合风险（适合车道线识别的轻量化需求）。

**缺点**

+ 死亡 ReLU 问题：若输入长期为负（如参数初始化不当、学习率过高），神经元梯度为 0，永远无法更新（“坏死”）；
+ 输出非零均值：输出均值为正，可能导致后续层输入偏移，影响训练稳定性；
+ 负区间无响应：完全忽略负特征，丢失部分语义信息（比如车道线边缘的负梯度特征）。

**适用场景**

+ 首选！CNN/UNet 的隐藏层（如你 U-Net 中卷积后的nn.ReLU）；
+ 点云感知、目标检测等大部分感知任务的中间层。

### Leaky ReLU —— ReLU 的 “修复版”

定义

$$
Leaky ReLU(x)= \begin{cases}
x, & x>0\\
αx,​ & x≤0
\end{cases}
$$

​α通常取 0.01（负区间保留微小梯度）。

**优点**

+ 解决「死亡 ReLU 问题」：负区间梯度为α（非 0），神经元可更新；
+ 继承 ReLU 的计算高效性，无额外开销。

**缺点**

+ α是手动设定的超参数，需调优（不同任务最优值不同）；
+ 负区间响应微弱，仍可能丢失部分特征。

**适用场景**

+ ReLU 出现 “死亡神经元” 时替换（比如你 U-Net 训练时损失停滞，可尝试）；
+ 高维数据（如点云特征）、深层网络（>10 层）。

### ELU（Exponential Linear Unit）—— 更平滑的 Leaky ReLU

**定义**

$$
ELU(x)=\begin{cases}
x,  ​& x>0 \\
α(ex−1), & x≤0
\end{cases}​
$$

**优点**

+ 解决「死亡 ReLU」+ 输出零均值（训练更稳定）；
+ 负区间平滑过渡，梯度更连续，拟合能力更强。

**缺点**

+ 计算略复杂（包含指数运算），速度比 ReLU 慢；
+ α仍需调优。

**适用场景**

+ 对训练稳定性要求高的场景（如车道线分割的小数据集训练）；
+ 不追求极致推理速度的离线感知任务。

### Swish —— Google 提出的 “ReLU 升级版”

**定义**

$$
Swish(x)=x⋅σ(x)
$$

σ(x)是 Sigmoid 函数（见下文），也可替换为 Hard-Swish（量化版）。

**优点**

+ 处处可导，梯度平滑，拟合能力优于 ReLU；
+ 无死亡神经元问题，训练更稳定；
+ Hard-Swish 版本计算高效，适合嵌入式部署（如机器人 Jetson 设备）。

**缺点**

+ 计算复杂度略高于 ReLU；
+ 理论解释性弱于 ReLU。

**适用场景**

+ 追求更高精度的感知任务（如车道线精细分割、3D 点云检测）；
+ 模型量化 / 嵌入式部署（选 Hard-Swish）。

### Sigmoid —— 二分类任务的 “专属激活”

**定义**

$$
Sigmoid(x)=\frac {1} {1+e^{−x}}
$$

​输出范围：(0,1)，可理解为 “概率值”。

**优点**

+ 输出有明确的概率意义（如车道线掩码的 “是否为车道线” 概率）；
+ 输出范围有限，易于梯度调整。

**缺点**

+ 梯度消失严重：|x|>4 时梯度趋近于 0，深层网络无法训练；
+ 计算复杂（指数运算）；
+ 输出非零均值，导致训练收敛慢。

**适用场景**

+ 二分类任务的输出层（如你 U-Net 的车道线掩码输出：nn.Sigmoid()，输出 0~1 的车道线概率）；
+ 注意力机制中的权重计算（如感知融合的多传感器权重）。

## Tanh（双曲正切）—— 零均值的 Sigmoid

**定义**

Tanh(x)=ex+e−xex−e−x​输出范围：(−1,1)，是 Sigmoid 的零均值版本。

**优点**

输出零均值，训练稳定性优于 Sigmoid；
比 Sigmoid 更 “敏感”，特征区分度更高。

缺点

仍存在梯度消失问题（|x|>3 时梯度趋近于 0）；
计算复杂度高。

适用场景

RNN/LSTM（时序感知任务，如动态目标跟踪）；
特征归一化要求高的场景（如传感器融合的特征对齐）。

### Softmax —— 多分类任务的输出层

**定义**

$$
Softmax(x_i​) = \frac {e^{x_i}} {\sum_{i=1}^{n}e^{x_j}}
$$

xi​​输出范围：(0,1)，所有类别输出和为 1（概率分布）。

**优点**

+ 输出为多分类的概率分布，可直接解读类别置信度；
+ 适配交叉熵损失，梯度计算简洁。

**缺点**

+ 对异常值敏感（某一维度值过大时，其他类别概率趋近于 0）；
+ 计算复杂度高（需计算所有维度的指数和）。

**适用场景**

+ 多类别分割 / 分类的输出层（如交通标识识别：红绿灯 / 停止线 / 车道线多分类）；
+ 感知任务的类别概率输出（如点云目标分类：行人 / 车辆 / 路沿）。

### GELU（Gaussian Error Linear Units）—— Transformer 的标配

**定义**

$$
GELU(x)=x⋅Φ(x)
$$

Φ(x)是高斯累积分布函数，近似为x⋅σ(1.702x)。

**优点**

结合了 ReLU 的稀疏性和 Sigmoid 的平滑性，拟合能力极强；
梯度更稳定，适合深层 Transformer 网络。

**缺点**

+ 计算复杂度高，推理速度慢；
+ 对小模型增益有限。

**适用场景**

+ Transformer 架构（如 BEV 感知、多模态融合的感知任务）；
+ 大模型（如视觉 Transformer）的感知任务