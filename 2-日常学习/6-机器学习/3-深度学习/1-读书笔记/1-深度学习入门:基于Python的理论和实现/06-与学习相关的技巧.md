## <font>参数的更新</font>
### <font>SGD 随机梯度下降法</font>
SGD的更新方式如下式
$$W \leftarrow \eta {\theta L \over \theta W}$$
这里把需要更新的权重参数记为$W$，把损失函数关于$W$的梯度记为 $\theta L \over \theta W$。  
$\eta$ 表示学习率，实际上会取 0.01 或 0.001 这些事先决定好的值。

虽然 SGD 简单，并且容易实现，但是在解决某些问题时可能没有效率。
这里，在指出 SGD 的缺点之际，我们来思考一下求下面这个函数的最小值
的问题。
$$f(x,y)={1\over20}x^2 + y^2$$

这个梯度的特征是，y 轴方向上大，x 轴方向上小。换句话说，
就是 y 轴方向的坡度大，而 x 轴方向的坡度小。这里需要注意的是，虽然式
（6.2）的最小值在 (x, y) = (0, 0) 处，但是图 6-2 中的梯度在很多地方并没有指
向 (0, 0)。

我们来尝试对图 6-1 这种形状的函数应用 SGD。从 (x, y) = (−7.0, 2.0) 处
（初始值）开始搜索，结果如图 6-3 所示。
在图 6-3 中，SGD 呈“之”字形移动。这是一个相当低效的路径。也就是说，
SGD 的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索
的路径就会非常低效。因此，我们需要比单纯朝梯度方向前进的 SGD 更聪
明的方法。SGD 低效的根本原因是，梯度的方向并没有指向最小值的方向

### <font color=deepskyblue>Momentum</font>
Momentum 是“动量”的意思，和物理有关。用数学式表示 Momentum 方
法，如下所示。
$$v \leftarrow \alpha v-\eta {\theta L \over \theta W}$$
$$W \leftarrow W + v$$
和前面的 SGD 一样，W 表示要更新的权重参数， 表示损失函数关
于 W 的梯度，η 表示学习率。这里新出现了一个变量 v，对应物理上的速度。
式（6.3）表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增
加这一物理法则。如图 6-4 所示，Momentum 方法给人的感觉就像是小球在
地面上滚动。

### <font color=deepskyblue>AdaGrad</font>
在神经网络的学习中，学习率（数学式中记为 η）的值很重要。学习率过小，
会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能
正确进行。

在关于学习率的有效技巧中，有一种被称为**学习率衰减**（learning rate
decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”
学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。

逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。
而 AdaGrad 进一步发展了这个想法，针对“一个一个”的参数，赋予其“定
制”的值。

AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习
（AdaGrad 的 Ada 来自英文单词 Adaptive，即“适当的”的意思）。下面，让
我们用数学式表示 AdaGrad 的更新方法。
$$h \leftarrow h+{\theta L \over \theta W} * {\theta L \over \theta W}$$