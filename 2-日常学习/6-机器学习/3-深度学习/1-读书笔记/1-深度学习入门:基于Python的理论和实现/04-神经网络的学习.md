## 数据驱动
神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指
可以由数据自动决定权重参数的值。

## <font color=coral>损失函数</font>

损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的
神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。

这个损失函数可以使用任意函数，
但一般用均方误差和交叉熵误差等。

### <font color=deepskyblue>均方误差</font>
可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared
error）。

$E = {1 \over 2}$ $\sum_k(y_k - t_k)^2$

这里，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据，k 表示数据的维数。

均方误差会计算神经网络的输出和正确解监督数据的
各个元素之差的平方，再求总和。现在，我们用 Python 来实现这个均方误差，
实现方式如下所示。
```
def mean_squared_error(y, t):
    return 0.5*np.sum((y-t)**2)
```
参数 y 和 t 是 NumPy 数组。

### <font color=deepskyblue>交叉熵误差</font>

交叉熵误差如下式所示：

$E = - \sum_kt_klog(y_k)$

**交叉熵误差的值是由正确解标签所对应的输出结果决定的**。
正确解标签对应的输出越大，交叉熵误差的值越接近 0；当输出为 1 时，交叉熵误差为 0。

## <font color=coral>mini-batch学习</font>
机器学习使用训练数据进行学习， 所谓的学习就是针对训练数据计算损失函数的值， 找出使该值尽可能小的参数

因此计算损失函数时必须将所有的训练数据作为对象， 前面的损失函数都是针对单个数据的损失函数， 如果是所有训练数据的损失函数的总和，则为下面的式子

$E = - { 1 \over N } \sum_1^n\sum_1^kt_{nk}log(y_{nk})$

这里假设 数据有 N 个， $t_{nk}表示第n个数据的第k个元素的值（$y_{nk}$是神经网络的输出， $t_{nk}$ 是监督数据）$

由于计算全部训练数据的时间过长， 我们在训练数据中随机挑选一批作为全部训练数据的近似， 这种方式被称为 mini-batch学习 （小批量）

以下是代码实现
```
import sys. os
sys.path
```

