- [数据驱动](#数据驱动)
- [<font color=coral>损失函数</font>](#font-colorcoral损失函数font)
  - [<font color=deepskyblue>均方误差</font>](#font-colordeepskyblue均方误差font)
  - [<font color=deepskyblue>交叉熵误差</font>](#font-colordeepskyblue交叉熵误差font)
- [<font color=coral>mini-batch学习</font>](#font-colorcoralmini-batch学习font)
  - [<font color=deepskyblue>mini-batch 交叉熵误差的实现</font>](#font-colordeepskybluemini-batch-交叉熵误差的实现font)
  - [<font color=deepskyblue>为何要设定损失函数</font>](#font-colordeepskyblue为何要设定损失函数font)
- [<font color=coral>数值微分</font>](#font-colorcoral数值微分font)
  - [<font color=deepskyblue>梯度法</font>](#font-colordeepskyblue梯度法font)


## 数据驱动
神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指
可以由数据自动决定权重参数的值。

## <font color=coral>损失函数</font>

损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的
神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。

这个损失函数可以使用任意函数，
但一般用均方误差和交叉熵误差等。

### <font color=deepskyblue>均方误差</font>
可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared
error）。

$E = {1 \over 2}$ $\sum_k(y_k - t_k)^2$

这里，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据，k 表示数据的维数。

均方误差会计算神经网络的输出和正确解监督数据的
各个元素之差的平方，再求总和。现在，我们用 Python 来实现这个均方误差，
实现方式如下所示。
```
def mean_squared_error(y, t):
    return 0.5*np.sum((y-t)**2)
```
参数 y 和 t 是 NumPy 数组。

### <font color=deepskyblue>交叉熵误差</font>

交叉熵误差如下式所示：

$E = - \sum_kt_klog(y_k)$

**交叉熵误差的值是由正确解标签所对应的输出结果决定的**。
正确解标签对应的输出越大，交叉熵误差的值越接近 0；当输出为 1 时，交叉熵误差为 0。

## <font color=coral>mini-batch学习</font>
机器学习使用训练数据进行学习， 所谓的学习就是针对训练数据计算损失函数的值， 找出使该值尽可能小的参数

因此计算损失函数时必须将所有的训练数据作为对象， 前面的损失函数都是针对单个数据的损失函数， 如果是所有训练数据的损失函数的总和，则为下面的式子

$E = - { 1 \over N } \sum_1^n\sum_1^kt_{nk}log(y_{nk})$

这里假设 数据有 N 个， $t_{nk}表示第n个数据的第k个元素的值（$y_{nk}$是神经网络的输出， $t_{nk}$ 是监督数据）$

由于计算全部训练数据的时间过长， 我们在训练数据中随机挑选一批作为全部训练数据的近似， 这种方式被称为 mini-batch学习 （小批量）

以下是代码实现
```python
import sys. os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
(x_train, t_train), (x_test, t_test) = \
load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

### <font color=deepskyblue>mini-batch 交叉熵误差的实现</font>

实现一个可以同时处理单个数据和批量数据的函数
```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y + 1e-7)) / batch_size
```

y 是神经网络的输出， t 是监督数据  
当 y 的维度为1时， 即求单个数据的交叉熵误差时， 需要改变数据的形状

当监督数据是标签形式（非 one-hot 表示）时， 交叉熵误差可以通过以下代码实现
```
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arrange(batch_size), t]+1e-7)) / batch_size
```

作为参考，简单介绍一下np.log( y[np.arange(batch_size), t] )。np.arange
(batch_size) 会生成一个从 0 到 batch_size-1 的数组。比如当 batch_size 为 5
时，np.arange(batch_size) 会生成一个 NumPy 数组 [0, 1, 2, 3, 4]。因为 t 中标签是以 [2, 7, 0, 9, 4] 的形式存储的，所以 y[np.arange(batch_size),
t] 能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中，
y[np.arange(batch_size), t] 会 生 成 NumPy 数 组 [y[0,2], y[1,7], y[2,0],
y[3,9], y[4,4]]）

### <font color=deepskyblue>为何要设定损失函数</font>
在神经网络的学习中，寻找最优参数（权重和偏置）时，
要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小
的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，
逐步更新参数的值。  
假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参
数。此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权
重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过使该权
重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，
则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数
的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此
时该权重参数的更新会停在此处。  
之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数
都会变为 0，导致参数无法更新。话说得有点多了，我们来总结一下上面的内容。

为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0呢？为了回答这个问题，我们来思考另一个具体例子。假设某个神经网络正
确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32 %。如果以识别精
度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会
出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精
度有所改善，它的值也不会像 32.0123 . . . % 这样连续变化，而是变为 33 %、
34 % 这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损
失函数的值可以表示为 0.92543 . . . 这样的值。并且，如果稍微改变一下参数
的值，对应的损失函数也会像 0.93432 . . . 这样发生连续性的变化。
识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值
也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出
于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。
如图 4-4 所示，阶跃函数的导数在绝大多数地方（除了 0 以外的地方）均为 0。
也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微
小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。
阶跃函数就像“竹筒敲石”一样，只在某个瞬间产生变化。而 sigmoid 函数，
如图 4-4 所示，不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）
也是连续变化的。也就是说，sigmoid 函数的导数在任何地方都不为 0。这对
神经网络的学习非常重要。得益于这个斜率不会为 0 的性质，神经网络的学
习得以正确进行。

## <font color=coral>数值微分</font>
梯度法使用梯度的信息决定前进的方向。梯度指示的方向
是各点处的函数值减小最多的方向
### <font color=deepskyblue>梯度法</font>
机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必
须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我
们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法
现在，我们尝试用数学式来表示梯度法，如式（4.7）所示。

$x_0 = x_0 - \eta{ \over n}$