- [数据驱动](#数据驱动)
- [<font color=coral>损失函数</font>](#font-colorcoral损失函数font)
  - [<font color=deepskyblue>均方误差</font>](#font-colordeepskyblue均方误差font)
  - [<font color=deepskyblue>交叉熵误差</font>](#font-colordeepskyblue交叉熵误差font)
- [<font color=coral>mini-batch学习</font>](#font-colorcoralmini-batch学习font)
  - [<font color=deepskyblue>mini-batch 交叉熵误差的实现</font>](#font-colordeepskybluemini-batch-交叉熵误差的实现font)
  - [<font color=deepskyblue>为何要设定损失函数</font>](#font-colordeepskyblue为何要设定损失函数font)
- [<font color=coral>数值微分</font>](#font-colorcoral数值微分font)
  - [<font color=deepskyblue>梯度法</font>](#font-colordeepskyblue梯度法font)
  - [<font color=deepskyblue>神经网络的梯度</font>](#font-colordeepskyblue神经网络的梯度font)
- [<font color=coral>学习算法的实现</font>](#font-colorcoral学习算法的实现font)
  - [<font color=deepskyblue>两层神经网络的实现</font>](#font-colordeepskyblue两层神经网络的实现font)
  - [<font color=deepskyblue>mini-batch的实现</font>](#font-colordeepskybluemini-batch的实现font)


## 数据驱动
神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指
可以由数据自动决定权重参数的值。

## <font color=coral>损失函数</font>

损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的
神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。

这个损失函数可以使用任意函数，
但一般用均方误差和交叉熵误差等。

### <font color=deepskyblue>均方误差</font>
可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared
error）。

$E = {1 \over 2}$ $\sum_k(y_k - t_k)^2$

这里，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据，k 表示数据的维数。

均方误差会计算神经网络的输出和正确解监督数据的
各个元素之差的平方，再求总和。现在，我们用 Python 来实现这个均方误差，
实现方式如下所示。
```
def mean_squared_error(y, t):
    return 0.5*np.sum((y-t)**2)
```
参数 y 和 t 是 NumPy 数组。

### <font color=deepskyblue>交叉熵误差</font>

交叉熵误差如下式所示：

$E = - \sum_kt_klog(y_k)$

**交叉熵误差的值是由正确解标签所对应的输出结果决定的**。
正确解标签对应的输出越大，交叉熵误差的值越接近 0；当输出为 1 时，交叉熵误差为 0。

## <font color=coral>mini-batch学习</font>
机器学习使用训练数据进行学习， 所谓的学习就是针对训练数据计算损失函数的值， 找出使该值尽可能小的参数

因此计算损失函数时必须将所有的训练数据作为对象， 前面的损失函数都是针对单个数据的损失函数， 如果是所有训练数据的损失函数的总和，则为下面的式子

$E = - { 1 \over N } \sum_1^n\sum_1^kt_{nk}log(y_{nk})$

这里假设 数据有 N 个， $t_{nk}表示第n个数据的第k个元素的值（$y_{nk}$是神经网络的输出， $t_{nk}$ 是监督数据）$

由于计算全部训练数据的时间过长， 我们在训练数据中随机挑选一批作为全部训练数据的近似， 这种方式被称为 mini-batch学习 （小批量）

以下是代码实现
```python
import sys. os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
(x_train, t_train), (x_test, t_test) = \
load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

### <font color=deepskyblue>mini-batch 交叉熵误差的实现</font>

实现一个可以同时处理单个数据和批量数据的函数
```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y + 1e-7)) / batch_size
```

y 是神经网络的输出， t 是监督数据  
当 y 的维度为1时， 即求单个数据的交叉熵误差时， 需要改变数据的形状

当监督数据是标签形式（非 one-hot 表示）时， 交叉熵误差可以通过以下代码实现
```
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arrange(batch_size), t]+1e-7)) / batch_size
```

作为参考，简单介绍一下np.log( y[np.arange(batch_size), t] )。np.arange
(batch_size) 会生成一个从 0 到 batch_size-1 的数组。比如当 batch_size 为 5
时，np.arange(batch_size) 会生成一个 NumPy 数组 [0, 1, 2, 3, 4]。因为 t 中标签是以 [2, 7, 0, 9, 4] 的形式存储的，所以 y[np.arange(batch_size),
t] 能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中，
y[np.arange(batch_size), t] 会 生 成 NumPy 数 组 [y[0,2], y[1,7], y[2,0],
y[3,9], y[4,4]]）

### <font color=deepskyblue>为何要设定损失函数</font>
在神经网络的学习中，寻找最优参数（权重和偏置）时，
要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小
的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，
逐步更新参数的值。  
假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参
数。此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权
重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过使该权
重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，
则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数
的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此
时该权重参数的更新会停在此处。  
之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数
都会变为 0，导致参数无法更新。话说得有点多了，我们来总结一下上面的内容。

为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0呢？为了回答这个问题，我们来思考另一个具体例子。假设某个神经网络正
确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32 %。如果以识别精
度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会
出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精
度有所改善，它的值也不会像 32.0123 . . . % 这样连续变化，而是变为 33 %、
34 % 这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损
失函数的值可以表示为 0.92543 . . . 这样的值。并且，如果稍微改变一下参数
的值，对应的损失函数也会像 0.93432 . . . 这样发生连续性的变化。
识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值
也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出
于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。
如图 4-4 所示，阶跃函数的导数在绝大多数地方（除了 0 以外的地方）均为 0。
也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微
小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。
阶跃函数就像“竹筒敲石”一样，只在某个瞬间产生变化。而 sigmoid 函数，
如图 4-4 所示，不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）
也是连续变化的。也就是说，sigmoid 函数的导数在任何地方都不为 0。这对
神经网络的学习非常重要。得益于这个斜率不会为 0 的性质，神经网络的学
习得以正确进行。

## <font color=coral>数值微分</font>
梯度法使用梯度的信息决定前进的方向。梯度指示的方向
是各点处的函数值减小最多的方向
### <font color=deepskyblue>梯度法</font>
机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必
须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法

这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。因此，
无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际
上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。

虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地
减小函数的值。因此，在寻找函数的最小值（或者尽可能小的值）的位置的
任务中，要以梯度的信息为线索，决定前进的方向。

$x_0 = x_0 - \eta{\theta f \over \theta x_0}$  
$x_1 = x_1 - \eta{\theta f \over \theta x_1}$

$\eta$ 表示更新量， 在神经网络的学习中， 称为学习率， 学习率决定一次学习中 ，应该学习多少， 以及在多大程度上更新参数  
上面的式子表示更新一次的步骤，这个步骤会反复执行。通过反复执行此步骤，逐渐减小函数值。
虽然这里只展示了有两个变量时的更新过程，但是即便增加变量的数量，也
可以通过类似的式子（各个变量的偏导数）进行更新。

```python
# f 是一个函数
def numerical_gradient(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # 生成和 x 形状相同的数组
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) 的计算
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h) 的计算
        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 还原值
    return grad

# 梯度下降法
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x  
```

使用这个函数可以求函数的极小值，顺利的话，还可以求函数的最小值。
下面，我们就来尝试解决下面这个问题。

### <font color=deepskyblue>神经网络的梯度</font>

神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参
数的梯度。比如，有一个只有一个形状为 2 × 3 的权重 W 的神经网络，损失
函数用 L 表示。此时，梯度可以用 表示。用数学式表示的话，如下所示。

$$W = \begin{pmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\ 
\end{pmatrix}$$


$${\theta L \over \theta W} = \begin{pmatrix}
{\theta L \over \theta w_{11}} & {\theta L \over \theta w_{12}} & {\theta L \over \theta w_{13}} \\
{\theta L \over \theta w_{21}} & {\theta L \over \theta w_{22}} & {\theta L \over \theta w_{23}} \\ 
\end{pmatrix}$$

下面，我们以一个简单的神经网络为例，来实现求梯度的代码。
```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 用高斯分布进行初始化

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        return loss
```

它有两个方法，一个是用于预
测的 predict(x)，另一个是用于求损失函数值的 loss(x,t)。这里参数 x 接收
输入数据，t 接收正确解标签。

```
>>> net = simpleNet()
>>> print(net.W) # 权重参数
[[ 0.47355232 0.9977393 0.84668094],
[ 0.85557411 0.03563661 0.69422093]])
>>>
>>> x = np.array([0.6, 0.9])
>>> p = net.predict(x)
>>> print(p)
[ 1.05414809 0.63071653 1.1328074]
>>> np.argmax(p) # 最大值的索引
2
>>>
>>> t = np.array([0, 0, 1]) # 正确解标签
>>> net.loss(x, t)
0.92806853663411326

>>> def f(W):
... return net.loss(x, t)
...
>>> dW = numerical_gradient(f, net.W)
>>> print(dW)
[[ 0.21924763 0.14356247 -0.36281009]
[ 0.32887144 0.2153437 -0.54421514]]
```

## <font color=coral>学习算法的实现</font>
+ **前提**  
  神经网络存在合适的偏重和配置， 调整权重和配置以便拟合训练数据的过程称为学习， 神经网络主要包括下面四个步骤
+ 步骤1（mini-batch）  
  从训练数据中挑选出一部分数据， 这部分数据称为 mini-batch， 我们的目标是减小mini-batch的损失函数的值
+ 步骤2（计算梯度）  
  为了减小 mini-batch 的损失函数的值， 需要求出各个权重参数的梯度， 梯度表示损失函数的值减小最多的方向
+ 步骤3（更新参数）
  将权重参数沿梯度方向 进行微小的更新
+ 重复 1， 2， 3

神经网络的学习按照上面 4 个步骤进行。这个方法通过梯度下降法更新
参数，不过因为这里使用的数据是随机选择的 mini batch 数据，所以又称为
随机梯度下降法（stochastic gradient descent）。“随机”指的是“随机选择的”
的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。
深度学习的很多框架中，随机梯度下降法一般由一个名为 SGD 的函数来实现。
SGD 来源于随机梯度下降法的英文名称的首字母。

### <font color=deepskyblue>两层神经网络的实现</font>
```python
import sys, os
sys.path.append(os.pardir)
from common.functions import *
from common.gradient import numerical_gradient
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
        weight_init_std=0.01):
        # 初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        return y

    # x: 输入数据 , t: 监督数据
    def loss(self, x, t):
        y = self.predict(x)
        return cross_entropy_error(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # x: 输入数据 , t: 监督数据
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads
```

### <font color=deepskyblue>mini-batch的实现</font>

