## 加权线性回归（weighted linear regression）的公式及其推理
### <font color="deepskyblue">1. 加权线性回归与普通线性回归</font>
对于一组有 N 个观测的数据  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(&nbsp; x<sub>i</sub>, &nbsp;y<sub>i</sub> &nbsp;), &nbsp;i = 1,2,3,...,N

可以使用一元线性回归模型  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y = ax + b + $\epsilon$

来拟合 x 与 y 之间的关系。 其中的参数 a, b 通常使用最小二乘拟合， 即寻找使代价函数  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J(a, b) = $1 \over N$ $\sum_1^n(y_i - ax_i - b)^2$  

最小的 a, b, 使得拟合曲线尽可能的接近所有的观测点  

但在实际应用中，观测点之间可能是有差异的。比如，有的观测点误差大，有的观测点误差小，这就需要让我们的拟合直线 y = ax + b, 不必考虑误差大的观测点， 而要尽可能逼近误差小的观测点。这时就可以使用一个权重系数 $w_i$ 来表示第 i 个观测点的权重（例如:对于误差小的观测点，$w_i$的值更大） 而考虑了这个权重系数$w_i$的线性回归，就是加权线性回归。  

它的回归方程仍然是  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y = ax + b + $\epsilon$  

唯一区别的代价函数变成了  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$J$(a, b) = $1 \over N$ $\sum_1^Nw_i(y_i - ax_i -b)^2$

这样， 在寻找最优a, b时， 会更多的考虑高权重的观测值

### <font color="deepskyblue">2.最优a,b及其求解</font>
代价函数  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$J$(a, b) = $1 \over N$ $\sum_1^N(w_i - ax_i -b)^2$  

是一个普通的二元二次函数， 分别将 J(a, b) 对 a, b求偏微分， 使这两个偏微分都为 0 即为最优解，  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\partial \over \partial a$$J(a, b)$ = $1 \over N$ [2 $\sum_1^Nw_ix_i(y_i - ax_i - b)$] = 0  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\partial \over \partial b$$J(a, b)$ = $1 \over N$ [2 $\sum_1^Nw_i(y_i - ax_i - b)$] = 0  

即可得  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\partial \over \partial a$ $J$(a, b) = $2 \over N$[$\sum_1^Nw_ix_iy_i$ - a$\sum_1^Nw_ix_i^2$ - b$\sum_1^Nw_ix_i$] = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\partial \over \partial a$ $J$(a, b) = $2 \over N$[$\sum_1^Nw_ix_iy_i$ - a$\sum_1^Nw_ix_i^2$ - b$\sum_1^Nw_ix_i$] = 0

为简化上述等式，定义以下符号  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\phi$<sub>$wxy$</sub> = $\sum_1^Nw_ix_iy_i$  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\phi$<sub>$wx^2$</sub> = $\sum_1^Nw_ix_i^2$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\phi$<sub>$wx$</sub> = $\sum_1^Nw_ix_i$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\phi$<sub>$wy$</sub> = $\sum_1^Nw_iy_i$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\phi$<sub>$w$</sub> = $\sum_1^Nw_i$

上式简化成  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a*$\phi$<sub>$wx^2$</sub> + b*$\phi$<sub>$wx$</sub> = $\phi$<sub>$wxy$</sub>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a*$\phi$<sub>$wx$</sub> + b*$\phi$<sub>$w$</sub> = $\phi$<sub>$wy$</sub>

解可得  

a = $\phi$<sub>$wxy$</sub>*$\phi$<sub>$wx$</sub> - $\phi$<sub>$wx_2$</sub>*$\phi$<sub>$wy$</sub> / $\phi$<sub>$wx$</sub><sup>2</sup> - $\phi$<sub>$w$</sub>*$\phi$<sub>$wx_2$</sub>

b = $\phi$<sub>$wy$</sub>*$\phi$<sub>$wx$</sub> - $\phi$<sub>$w$</sub>*$\phi$<sub>$wy$</sub> / $\phi$<sub>$wx$</sub><sup>2</sup> - $\phi$<sub>$w$</sub>*$\phi$<sub>$wx_2$</sub>

如果 令 $w_i$ = 1, 则以上公式与普通相性回归参数估计公式相同

### <font color="deepskyblue">3. $R^2$ 与 F 检验</font>
对于普通的线性回归模型， $R^2$ 的计算公式如下：

$R^2$ =  1 - $\sum_1^N$$(y_i-ax_i-b)^2$/$\sum_1^N$($y_i$ - $1 \over N$$\sum1^Ny_j$)


